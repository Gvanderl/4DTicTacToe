{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "meNNqqqlRlN4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "from agent import Agent, advanced_random_policy, random_policy, sarsa\n",
    "from game import Game\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a first time, we train an agent for 2D tic-tac-toe using Q-learning with our modified SARSA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:34<00:00, 129.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "game = Game(None, None, n_dim=2, size=3)\n",
    "agent = Agent(size=3)\n",
    "\n",
    "n_eps = 20000\n",
    "# Trains agent with the random policy\n",
    "sarsa(game, agent, random_policy, alpha=0.45, alpha_factor=0.9995**(10000/n_eps), gamma=0.7, epsilon=1.0, \\\n",
    "      epsilon_factor=0.9997**(10000/n_eps), r_win=11.0, r_lose=0.0, r_even=1.0, r_even2=1.25, num_episodes=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make our agent play 1000 games against the random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 1000 games, agent won 890 times, lost 0 times and made 110 even games.\n"
     ]
    }
   ],
   "source": [
    "random_opponent = Agent(size=3, policy=random_policy)\n",
    "game = Game(agent, random_opponent, n_dim=2, size=3)\n",
    "\n",
    "agent_1_win, oppo_2_win, agent_1_even, agent_2_win, oppo_1_win, agent_2_even = game.simulate_games(1000)\n",
    "tot_agent_win = agent_1_win + agent_2_win\n",
    "tot_agent_lose = oppo_1_win + oppo_2_win\n",
    "tot_even = agent_1_even + agent_2_even\n",
    "print(\"On 1000 games, agent won\", tot_agent_win, \"times, lost\", tot_agent_lose, \"times and made\", tot_even, \"even games.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make our agent play 1000 games against the advanced policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 1000 games, agent won 244 times, lost 7 times and made 749 even games.\n"
     ]
    }
   ],
   "source": [
    "advanced_opponent = Agent(size=3, policy=advanced_random_policy)\n",
    "game = Game(agent, advanced_opponent, n_dim=2, size=3)\n",
    "\n",
    "agent_1_win, oppo_2_win, agent_1_even, agent_2_win, oppo_1_win, agent_2_even = game.simulate_games(1000)\n",
    "tot_agent_win = agent_1_win + agent_2_win\n",
    "tot_agent_lose = oppo_1_win + oppo_2_win\n",
    "tot_even = agent_1_even + agent_2_even\n",
    "print(\"On 1000 games, agent won\", tot_agent_win, \"times, lost\", tot_agent_lose, \"times and made\", tot_even, \"even games.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give the possibility to play against our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . \n",
      ". . . \n",
      ". . . \n",
      "\n",
      "Agent plays : (1, 1) \n",
      "\n",
      ". . . \n",
      ". X . \n",
      ". . . \n",
      "\n",
      "Coordinates of next move : 0 2\n",
      "\n",
      ". . O \n",
      ". X . \n",
      ". . . \n",
      "\n",
      "Agent plays : (0, 1) \n",
      "\n",
      ". X O \n",
      ". X . \n",
      ". . . \n",
      "\n",
      "Coordinates of next move : 21\n",
      "\n",
      ". X O \n",
      ". X . \n",
      ". O . \n",
      "\n",
      "Agent plays : (1, 2) \n",
      "\n",
      ". X O \n",
      ". X X \n",
      ". O . \n",
      "\n",
      "Coordinates of next move : 1 - 0\n",
      "\n",
      ". X O \n",
      "O X X \n",
      ". O . \n",
      "\n",
      "Agent plays : (0, 0) \n",
      "\n",
      "X X O \n",
      "O X X \n",
      ". O . \n",
      "\n",
      "Coordinates of next move : 2, 2\n",
      "\n",
      "X X O \n",
      "O X X \n",
      ". O O \n",
      "\n",
      "Agent plays : (2, 0) \n",
      "\n",
      "X X O \n",
      "O X X \n",
      "X O O \n",
      "\n",
      "Game over. Score : (0, 0)\n",
      "Even score.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game(agent, \"Human player\", n_dim=2, size=3) # agent plays first\n",
    "#game = Game(\"Human player\", agent, n_dim=2, size=3) # to play first\n",
    "game.play_a_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the performances of our agent depending on the number of training episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.91it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 90.65it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 94.68it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 101.83it/s]\n",
      "100%|██████████| 400/400 [00:03<00:00, 104.12it/s]\n",
      "100%|██████████| 700/700 [00:06<00:00, 107.10it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 112.96it/s]\n",
      "100%|██████████| 2000/2000 [00:16<00:00, 121.59it/s]\n",
      "100%|██████████| 3000/3000 [00:24<00:00, 122.57it/s]\n"
     ]
    }
   ],
   "source": [
    "win, lose, draw, n_eps = [], [], [], []\n",
    "for n in [1, 10, 100, 200, 400, 700, 1000, 2000, 3000, 4000, 5000, 7000, 10000, 15000, 20000]:\n",
    "    ar = 0.9995**(10000/n)\n",
    "    er = 0.9997**(10000/n)\n",
    "    agent = Agent(size=3)\n",
    "    random_opponent = Agent(size=3, policy=advanced_random_policy)\n",
    "    game = Game(agent, random_opponent, n_dim=2, size=3)\n",
    "    sarsa(game, agent, random_policy, alpha=0.45, alpha_factor=ar, gamma=0.7, epsilon=1.0, epsilon_factor=er, \\\n",
    "          r_win=11.0, r_lose=0.0, r_even=1.0, r_even2=1.25, num_episodes=n)\n",
    "    win_p1_a, win_p2_a, tot_even_a, win_p1_b, win_p2_b, tot_even_b = game.simulate_games(10000)\n",
    "    win.append(win_p1_a + win_p1_b)\n",
    "    lose.append(win_p2_a + win_p2_b)\n",
    "    draw.append(tot_even_a + tot_even_b)\n",
    "    n_eps.append(n)\n",
    "    \n",
    "n_eps = np.array(n_eps)\n",
    "fig = plt.figure(figsize=(0.7*6.4, 0.7*4.8))\n",
    "plt.plot(n_eps, np.array(win)/100, label='win')\n",
    "plt.plot(n_eps, np.array(lose)/100, label='lose')\n",
    "plt.plot(n_eps, np.array(draw)/100, label='draw')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('number of episodes played in training')\n",
    "plt.ylabel('test games (in %)')\n",
    "plt.ylim(0, 105)\n",
    "plt.title('Performances when training\\nagainst the random policy and\\nplaying against the random policy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we show our Policy Gradient model for the 2D and 3D case\n",
    "We start by the 2D case\n",
    "\n",
    "We train our model with 10k iteration and batch size of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:43<00:00, 231.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(0.20265856, dtype=float32), array(0.20265856, dtype=float32), array(0.2505353, dtype=float32), array(0.2505353, dtype=float32), array(0.24982487, dtype=float32), array(0.24982487, dtype=float32), array(0.23770878, dtype=float32), array(0.23770878, dtype=float32), array(0.23626584, dtype=float32), array(0.23626584, dtype=float32), array(0.23059121, dtype=float32), array(0.23059121, dtype=float32), array(0.2311154, dtype=float32), array(0.2311154, dtype=float32), array(0.22827327, dtype=float32), array(0.22827327, dtype=float32), array(0.2278862, dtype=float32), array(0.2278862, dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import neuralAgent as na\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "from game import Game\n",
    "\n",
    "game = Game(None, None, n_dim=2, size=3)\n",
    "agent1 = na.Model()\n",
    "agent1, values, _, _, _ = na.train_network(agent1, game, 10000, 1000)\n",
    "agent1.save()\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make our agent play 1000 games and then 10000 against the random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 221.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win 880, Draw 24, Loses 96\n"
     ]
    }
   ],
   "source": [
    "# Example of loading a saved agent\n",
    "loaded_agent = na.Model()\n",
    "loaded_agent.load()\n",
    "wins, draw, loses = na.test_against_random(agent1, game, 1000)\n",
    "print(\"Win {}, Draw {}, Loses {}\".format(wins, draw, loses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:43<00:00, 227.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win 8668, Draw 270, Loses 1062\n"
     ]
    }
   ],
   "source": [
    "wins, draw, loses = na.test_against_random(agent1, game, 10000)\n",
    "print(\"Win {}, Draw {}, Loses {}\".format(wins, draw, loses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . \n",
      ". . . \n",
      ". . . \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_best_possible_from_board() missing 2 required positional arguments: 'size' and 'n_dim'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-42e72be0686e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mgame\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mGame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0magent1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Human player\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_dim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# agent plays first\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m#game = Game(\"Human player\", agent, n_dim=2, size=3) # to play first\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mgame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplay_a_game\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/run/media/roundedglint585/NewVolume/Study/Master/INF581/finalProject/CrossDimNoughts/game.py\u001B[0m in \u001B[0;36mplay_a_game\u001B[0;34m(self, verbose)\u001B[0m\n\u001B[1;32m     53\u001B[0m                 \u001B[0mplayer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mp2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mplayer\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m\"Human player\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 55\u001B[0;31m                 \u001B[0mcoords\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplay_vs_opponent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mturn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     56\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m                     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Agent plays :'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcoords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'\\n'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/run/media/roundedglint585/NewVolume/Study/Master/INF581/finalProject/CrossDimNoughts/neuralAgent.py\u001B[0m in \u001B[0;36mplay_vs_opponent\u001B[0;34m(self, board, turn)\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[0mnew_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_done\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgame_instance\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbest_move_tuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mprev_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbest_move\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 59\u001B[0;31m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[0;31m#\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: get_best_possible_from_board() missing 2 required positional arguments: 'size' and 'n_dim'"
     ]
    }
   ],
   "source": [
    "game = Game(agent1, \"Human player\", n_dim=2, size=3) # agent plays first\n",
    "#game = Game(\"Human player\", agent, n_dim=2, size=3) # to play first\n",
    "game.play_a_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the 3D case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [10:37<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(0.2168715, dtype=float32), array(0.2168715, dtype=float32), array(0.2672852, dtype=float32), array(0.2672852, dtype=float32), array(0.28510368, dtype=float32), array(0.28510368, dtype=float32), array(0.26848608, dtype=float32), array(0.26848608, dtype=float32), array(0.31130293, dtype=float32), array(0.31130293, dtype=float32), array(0.30502424, dtype=float32), array(0.30502424, dtype=float32), array(0.3202326, dtype=float32), array(0.3202326, dtype=float32), array(0.34884772, dtype=float32), array(0.34884772, dtype=float32), array(0.36489576, dtype=float32), array(0.36489576, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "game = Game(None, None, n_dim=3, size=3)\n",
    "agent1 = na.Model(3, 3)\n",
    "agent1, values, _, _, _ =na.train_network(agent1, game, 10000, 1000)\n",
    "#print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show its performances against the random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:58<00:00, 16.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win 964, Draw 25, Loses 11\n"
     ]
    }
   ],
   "source": [
    "wins, draw, loses = na.test_against_random(agent1, game, 1000)\n",
    "print(\"Win {}, Draw {}, Loses {}\".format(wins, draw, loses))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RL_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}