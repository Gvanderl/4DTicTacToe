{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "meNNqqqlRlN4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "from agent import Agent, advanced_random_policy, random_policy, sarsa\n",
    "from game import Game\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a first time, we train an agent for 2D tic-tac-toe using our modified SARSA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:22<00:00, 140.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 23s, sys: 6.24 s, total: 2min 29s\n",
      "Wall time: 2min 22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "game = Game(None, None, n_dim=2, size=3)\n",
    "agent = Agent(size=3)\n",
    "\n",
    "n_eps = 20000\n",
    "# Trains agent with the advanced policy\n",
    "sarsa(game, agent, advanced_random_policy, alpha=0.45, alpha_factor=0.9995**(10000/n_eps), gamma=0.7, epsilon=1.0, \\\n",
    "      epsilon_factor=0.9997**(10000/n_eps), r_win=11.0, r_lose=0.0, r_even=1.0, r_even2=1.25, num_episodes=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make our agent play 1000 games against the random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 1000 games, agent won 895 times, lost 0 times and made 105 even games.\n"
     ]
    }
   ],
   "source": [
    "random_opponent = Agent(size=3, policy=random_policy)\n",
    "game = Game(agent, random_opponent, n_dim=2, size=3)\n",
    "\n",
    "agent_1_win, oppo_2_win, agent_1_even, agent_2_win, oppo_1_win, agent_2_even = game.simulate_games(1000)\n",
    "tot_agent_win = agent_1_win + agent_2_win\n",
    "tot_agent_lose = oppo_1_win + oppo_2_win\n",
    "tot_even = agent_1_even + agent_2_even\n",
    "print(\"On 1000 games, agent won\", tot_agent_win, \"times, lost\", tot_agent_lose, \"times and made\", tot_even, \"even games.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make our agent play 1000 games against the advanced policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 1000 games, agent won 253 times, lost 0 times and made 747 even games.\n"
     ]
    }
   ],
   "source": [
    "advanced_opponent = Agent(size=3, policy=advanced_random_policy)\n",
    "game = Game(agent, advanced_opponent, n_dim=2, size=3)\n",
    "\n",
    "agent_1_win, oppo_2_win, agent_1_even, agent_2_win, oppo_1_win, agent_2_even = game.simulate_games(1000)\n",
    "tot_agent_win = agent_1_win + agent_2_win\n",
    "tot_agent_lose = oppo_1_win + oppo_2_win\n",
    "tot_even = agent_1_even + agent_2_even\n",
    "print(\"On 1000 games, agent won\", tot_agent_win, \"times, lost\", tot_agent_lose, \"times and made\", tot_even, \"even games.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give the possibility to play against our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . \n",
      ". . . \n",
      ". . . \n",
      "\n",
      "Agent plays : (1, 1) \n",
      "\n",
      ". . . \n",
      ". X . \n",
      ". . . \n",
      "\n",
      "Coordinates of next move : 0 0\n",
      "\n",
      "O . . \n",
      ". X . \n",
      ". . . \n",
      "\n",
      "Agent plays : (0, 1) \n",
      "\n",
      "O X . \n",
      ". X . \n",
      ". . . \n",
      "\n",
      "Coordinates of next move : 2 1\n",
      "\n",
      "O X . \n",
      ". X . \n",
      ". O . \n",
      "\n",
      "Agent plays : (1, 2) \n",
      "\n",
      "O X . \n",
      ". X X \n",
      ". O . \n",
      "\n",
      "Coordinates of next move : 1 0\n",
      "\n",
      "O X . \n",
      "O X X \n",
      ". O . \n",
      "\n",
      "Agent plays : (2, 0) \n",
      "\n",
      "O X . \n",
      "O X X \n",
      "X O . \n",
      "\n",
      "Coordinates of next move : 0 2\n",
      "\n",
      "O X O \n",
      "O X X \n",
      "X O . \n",
      "\n",
      "Agent plays : (2, 2) \n",
      "\n",
      "O X O \n",
      "O X X \n",
      "X O X \n",
      "\n",
      "Game over. Score : (0, 0)\n",
      "Even score.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game(agent, \"Human player\", n_dim=2, size=3) # agent plays first\n",
    "#game = Game(\"Human player\", agent, n_dim=2, size=3) # to play first\n",
    "game.play_a_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the performances of our agent depending on the number of training episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "win1, lose1, draw1, win2, lose2, draw2, n_eps = [], [], [], [], [], [], []\n",
    "for n in [1, 10, 100, 200, 400, 700, 1000, 2000, 3000, 4000, 5000, 7000, 10000, 15000, 20000]:\n",
    "    ar = 0.9995**(10000/n)\n",
    "    er = 0.9997**(10000/n)\n",
    "    agent = Agent(size=3)\n",
    "    advanced_opponent = Agent(size=3, policy=advanced_random_policy)\n",
    "    game = Game(agent, advanced_opponent, n_dim=2, size=3)\n",
    "    sarsa(game, agent, random_policy, alpha=0.45, alpha_factor=ar, gamma=0.7, epsilon=1.0, epsilon_factor=er, \\\n",
    "          r_win=11.0, r_lose=0.0, r_even=1.0, r_even2=1.25, num_episodes=n)\n",
    "    win_p1_a, win_p2_a, tot_even_a, win_p1_b, win_p2_b, tot_even_b = game.simulate_games(10000)\n",
    "    win1.append(win_p1_a)\n",
    "    win2.append(win_p1_b)\n",
    "    lose1.append(win_p2_a)\n",
    "    lose2.append(win_p2_b)\n",
    "    draw1.append(tot_even_a)\n",
    "    draw2.append(tot_even_b)\n",
    "    n_eps.append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results when the agent plays in first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eps = np.array(n_eps)\n",
    "fig = plt.figure(figsize=(0.6*6.4, 1*4.8))\n",
    "plt.plot(n_eps, np.array(win1)/50, label='win')\n",
    "plt.plot(n_eps, np.array(lose1)/50, label='lose')\n",
    "plt.plot(n_eps, np.array(draw1)/50, label='draw')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('number of episodes played in training', fontsize='large')\n",
    "plt.ylabel('test games (in % on 5000 games)', fontsize='large')\n",
    "plt.ylim(0, 105)\n",
    "plt.title('Fig.(7) : Training with the random\\npolicy and testing against the\\nadvanced one (agent plays in first)')\n",
    "plt.legend(loc='best', fontsize='large')\n",
    "plt.savefig(\"plot1.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results when the agent plays in second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eps = np.array(n_eps)\n",
    "fig = plt.figure(figsize=(0.6*6.4, 1*4.8))\n",
    "plt.plot(n_eps, np.array(win2)/50, label='win')\n",
    "plt.plot(n_eps, np.array(lose2)/50, label='lose')\n",
    "plt.plot(n_eps, np.array(draw2)/50, label='draw')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('number of episodes played in training', fontsize='large')\n",
    "plt.ylabel('test games (in % on 5000 games)', fontsize='large')\n",
    "plt.ylim(0, 105)\n",
    "plt.title('Fig.(8) : Training with the random\\npolicy and testing against the\\nadvanced one (agent plays in second)')\n",
    "plt.legend(loc='best', fontsize='large')\n",
    "plt.savefig(\"plot2.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we show our Policy Gradient model for 2D, 3D and 4D\n",
    "We start by the 2D case\n",
    "\n",
    "We train our model with 20 000 iteration and batch size of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:32<00:00, 216.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(0.1919922, dtype=float32), array(0.1919922, dtype=float32), array(0.23683336, dtype=float32), array(0.23683336, dtype=float32), array(0.24077186, dtype=float32), array(0.24077186, dtype=float32), array(0.23229553, dtype=float32), array(0.23229553, dtype=float32), array(0.21925719, dtype=float32), array(0.21925719, dtype=float32), array(0.23385824, dtype=float32), array(0.23385824, dtype=float32), array(0.2421333, dtype=float32), array(0.2421333, dtype=float32), array(0.22941184, dtype=float32), array(0.22941184, dtype=float32), array(0.23332284, dtype=float32), array(0.23332284, dtype=float32), array(0.22402705, dtype=float32), array(0.22402705, dtype=float32), array(0.22560924, dtype=float32), array(0.22560924, dtype=float32), array(0.23400809, dtype=float32), array(0.23400809, dtype=float32), array(0.22613238, dtype=float32), array(0.22613238, dtype=float32), array(0.22953445, dtype=float32), array(0.22953445, dtype=float32), array(0.22006044, dtype=float32), array(0.22006044, dtype=float32), array(0.22741458, dtype=float32), array(0.22741458, dtype=float32), array(0.22637656, dtype=float32), array(0.22637656, dtype=float32), array(0.22459088, dtype=float32), array(0.22459088, dtype=float32), array(0.23706421, dtype=float32), array(0.23706421, dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import neuralAgent as na\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "\n",
    "game = Game(None, None, n_dim=2, size=3)\n",
    "agent1 = na.Model()\n",
    "agent1, values, _, _, _ = na.train_network(agent1, game, 20000, 1000)\n",
    "agent1.save()\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make our agent play 1000 games and then 10000 against the random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 220.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win 850, Draw 38, Loses 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of loading a saved agent\n",
    "loaded_agent = na.Model()\n",
    "loaded_agent.load()\n",
    "wins, draw, loses = na.test_against_random(agent1, game, 1000)\n",
    "print(\"Win {}, Draw {}, Loses {}\".format(wins, draw, loses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:42<00:00, 233.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win 8770, Draw 291, Loses 939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wins, draw, loses = na.test_against_random(agent1, game, 10000)\n",
    "print(\"Win {}, Draw {}, Loses {}\".format(wins, draw, loses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . \n",
      ". . . \n",
      ". . . \n",
      "\n",
      "Agent plays : (0, 2) \n",
      "\n",
      ". . X \n",
      ". . . \n",
      ". . . \n",
      "\n",
      "Coordinates of next move : 1 1\n",
      "\n",
      ". . X \n",
      ". O . \n",
      ". . . \n",
      "\n",
      "Agent plays : (1, 2) \n",
      "\n",
      ". . X \n",
      ". O X \n",
      ". . . \n",
      "\n",
      "Coordinates of next move : 2 2\n",
      "\n",
      ". . X \n",
      ". O X \n",
      ". . O \n",
      "\n",
      "Agent plays : (2, 0) \n",
      "\n",
      ". . X \n",
      ". O X \n",
      "X . O \n",
      "\n",
      "Coordinates of next move : 0 0\n",
      "\n",
      "O . X \n",
      ". O X \n",
      "X . O \n",
      "\n",
      "Game over. Score : (0, 1)\n",
      "Human player wins !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game(agent1, \"Human player\", n_dim=2, size=3) # agent plays first\n",
    "#game = Game(\"Human player\", agent1, n_dim=2, size=3) # to play first\n",
    "game.play_a_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the 3D case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [10:23<00:00, 16.03it/s]\n"
     ]
    }
   ],
   "source": [
    "game = Game(None, None, n_dim=3, size=3)\n",
    "agent1 = na.Model(3, 3)\n",
    "agent1, values, _, _, _ = na.train_network(agent1, game, 10000, 1000)\n",
    "#print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show its performances against the random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:01<00:00, 16.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win 912, Draw 39, Loses 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wins, draw, loses = na.test_against_random(agent1, game, 1000)\n",
    "print(\"Win {}, Draw {}, Loses {}\".format(wins, draw, loses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RL_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
