{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "meNNqqqlRlN4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sympy.utilities.iterables import subsets\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "import pandas as pd\n",
    "import gym\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "\n",
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_almost_align(board, s):\n",
    "    size = board.shape[0]\n",
    "    for irow in range(size):\n",
    "        if board[irow, :].sum() == s * (size - 1):\n",
    "            i_empty = np.where(board[irow, :] == 0)[0].item()\n",
    "            return (irow, i_empty)\n",
    "    for icol in range(size):\n",
    "        if board[:, icol].sum() == s * (size - 1):\n",
    "            i_empty = np.where(board[:, icol] == 0)[0].item()\n",
    "            return (i_empty, icol)\n",
    "    if np.diag(board).sum() == s * (size - 1):\n",
    "        i_empty = np.where(np.diag(board) == 0)[0].item()\n",
    "        return (i_empty, i_empty)\n",
    "    if np.diag(np.rot90(board)).sum() == s * (size - 1):\n",
    "        for i in range(size):\n",
    "            if board[i, size - 1 - i] == 0:\n",
    "                return (i, size - 1 - i)\n",
    "    return None\n",
    "\n",
    "def random_policy(board, symbol):\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    pos = np.random.randint(available_pos.shape[0])\n",
    "    return tuple(available_pos[np.random.randint(available_pos.shape[0])])\n",
    "\n",
    "def linear_policy(board, symbol):\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    return tuple(available_pos[0])\n",
    "\n",
    "def advanced_static_policy(board, symbol):\n",
    "    coords = check_almost_align(board, symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    coords = check_almost_align(board, -symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    return tuple(available_pos[0])\n",
    "\n",
    "def advanced_random_policy(board, symbol):\n",
    "    coords = check_almost_align(board, symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    coords = check_almost_align(board, -symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    return tuple(available_pos[np.random.randint(available_pos.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_Wavup5eRlOB"
   },
   "outputs": [],
   "source": [
    "def sarsa(game, agent, opponent_policy, alpha, alpha_factor, gamma, epsilon, epsilon_factor,\n",
    "          r_win, r_lose, r_even, r_even2, num_episodes):\n",
    "    for episode_index in range(num_episodes):\n",
    "        if episode_index % 1000 == 0:\n",
    "            print('.', end='')\n",
    "        alpha *= alpha_factor\n",
    "        epsilon *= epsilon_factor\n",
    "        state = game.reset()\n",
    "        if episode_index%2 == 1:\n",
    "            action = opponent_policy(state, game.turn)\n",
    "            state, _, _, _ = game.step(action)\n",
    "        action = agent.epsilon_greedy_policy(state, epsilon)\n",
    "        state_history = [state.copy()]\n",
    "        action_history = [action]\n",
    "        while True:\n",
    "            ################### The agent ignores this happens UNLESS it ends the game\n",
    "            intermediate_state, reward, done, _ = game.step(action)\n",
    "            if done:\n",
    "                if reward > 0:\n",
    "                    final_reward = r_win\n",
    "                else:\n",
    "                    final_reward = r_even\n",
    "                break\n",
    "            intermediate_action = opponent_policy(intermediate_state, game.turn)\n",
    "            ################### ------------------------------------------------\n",
    "            state, reward, done, _ = game.step(intermediate_action)\n",
    "            if done:\n",
    "                if reward > 0:\n",
    "                    reward = r_lose\n",
    "                else:\n",
    "                    reward = r_even2\n",
    "                break\n",
    "            action = agent.epsilon_greedy_policy(state, epsilon)\n",
    "            state_history.append(state.copy())\n",
    "            action_history.append(action)\n",
    "            \n",
    "        agent.update_Qtable(state_history, action_history, reward, alpha, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yi4p1W_wRlOC"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, size=3, policy=None):\n",
    "        self.q_array = pd.DataFrame(columns=[str(i)+str(j) for i in range(size) for j in range(size)])\n",
    "        self.policy = policy\n",
    "        self.identity = {}\n",
    "        self.convert_rot90_1 = {}\n",
    "        self.convert_rot90_2 = {}\n",
    "        self.convert_rot90_3 = {}\n",
    "        self.convert_horizontal_axis = {}\n",
    "        self.convert_vertical_axis = {}\n",
    "        self.convert_diag = {}\n",
    "        self.convert_antidiag = {}\n",
    "        empty = np.zeros([size]*2, dtype=int)\n",
    "        \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = empty\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.identity[str(i)+str(j)] = coords_transform\n",
    "        \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = np.rot90(empty, 1)\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_rot90_1[str(i)+str(j)] = coords_transform\n",
    "        \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = np.rot90(empty, 2)\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_rot90_2[str(i)+str(j)] = coords_transform\n",
    "                \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = np.rot90(empty, 3)\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_rot90_3[str(i)+str(j)] = coords_transform\n",
    "                \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = empty[::-1]\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_horizontal_axis[str(i)+str(j)] = coords_transform\n",
    "        \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = empty[:, ::-1]\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_vertical_axis[str(i)+str(j)] = coords_transform\n",
    "        \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = empty.T\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_diag[str(i)+str(j)] = coords_transform\n",
    "                \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = np.rot90(empty, 2).T\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_antidiag[str(i)+str(j)] = coords_transform\n",
    "                \n",
    "    \n",
    "    def update_Qtable(self, state_history, action_history, reward, alpha, gamma): \n",
    "        code_last_state, code_last_action = self.encode_state_and_action(state_history[-1], action_history[-1])\n",
    "        try:\n",
    "            self.q_array.loc[code_last_state]\n",
    "        except:\n",
    "            self.q_array.loc[code_last_state] = 0.5\n",
    "        self.q_array.loc[code_last_state, code_last_action] = reward\n",
    "        \n",
    "        for i in range(len(state_history)-2, -1, -1):\n",
    "            state = state_history[i]\n",
    "            new_state = state_history[i+1]\n",
    "            action = action_history[i]\n",
    "            new_action = action_history[i+1]\n",
    "            code_state, code_action = self.encode_state_and_action(state, action)\n",
    "            try:\n",
    "                self.q_array.loc[code_state]\n",
    "            except:\n",
    "                self.q_array.loc[code_state] = 0.5\n",
    "            code_new_state, code_new_action = self.encode_state_and_action(new_state, new_action)\n",
    "            self.q_array.loc[code_state, code_action] = (1 - alpha) * self.q_array.loc[code_state, code_action] \\\n",
    "                                                        + alpha * gamma * self.q_array.loc[code_new_state, code_new_action]\n",
    "    \n",
    "    def play_vs_opponent(self, state, symbol):\n",
    "        if self.policy is not None:\n",
    "            return self.policy(state, symbol)\n",
    "        state_code = self.encode_state(state)\n",
    "        try:\n",
    "            self.q_array.loc[state_code]\n",
    "        except:\n",
    "            legal_moves = np.argwhere(state == 0)\n",
    "            return tuple(legal_moves[np.random.randint(legal_moves.shape[0])])\n",
    "        ###### Find best move\n",
    "        potential_actions = self.q_array.loc[state_code]\n",
    "        reference_state = self.decode_one_state(state_code)\n",
    "        free_spots_scores = []\n",
    "        \n",
    "        if np.prod(state == reference_state):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.identity[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == np.rot90(reference_state, 1)):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_rot90_1[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == np.rot90(reference_state, 2)):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_rot90_2[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == np.rot90(reference_state, 3)):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_rot90_3[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == reference_state[::-1]):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_horizontal_axis[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == reference_state[:, ::-1]):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_vertical_axis[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == reference_state.T):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_diag[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == np.rot90(reference_state, 2).T):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_antidiag[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        else:\n",
    "            print('Error in play_vs_opponent in Agent !!!')\n",
    "        max_reward = max(free_spots_scores, key=lambda x: x[1])[1]\n",
    "        best_actions = [act for act, rew in free_spots_scores if rew == max_reward]\n",
    "        return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "    def greedy_policy(self, state):\n",
    "        \"\"\"\n",
    "        Return the next action as a tuple\n",
    "        \"\"\"\n",
    "        code_state = self.encode_state(state)\n",
    "        try:\n",
    "            self.q_array.loc[code_state]\n",
    "        except:\n",
    "            self.q_array.loc[code_state] = 0.5\n",
    "        reference_state = self.decode_one_state(code_state)\n",
    "        legal_actions = []\n",
    "        actions = self.q_array.loc[code_state]\n",
    "            \n",
    "        if np.prod(state == reference_state):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.identity[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == np.rot90(reference_state, 1)):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_rot90_1[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == np.rot90(reference_state, 2)):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_rot90_2[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == np.rot90(reference_state, 3)):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_rot90_3[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == reference_state[::-1]):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_horizontal_axis[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == reference_state[:, ::-1]):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_vertical_axis[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == reference_state.T):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_diag[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == np.rot90(reference_state, 2).T):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_antidiag[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        print('\\nError in greedy policy !!!\\n')\n",
    "        print(state, '\\n')\n",
    "        print(reference_state, '\\n')\n",
    "        print(reference_state[::-1][::-1].T)\n",
    "\n",
    "    def epsilon_greedy_policy(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Return the next action as a tuple\n",
    "        \"\"\"\n",
    "        if self.q_array.empty or np.random.rand() < epsilon:\n",
    "            legal_moves = np.argwhere(state == 0)\n",
    "            size = legal_moves.shape[0]\n",
    "            random_idx = np.random.randint(size)\n",
    "            action = tuple(legal_moves[random_idx])\n",
    "        else:\n",
    "            action = self.greedy_policy(state)\n",
    "        return action\n",
    "    \n",
    "    def encode_action(self, action):\n",
    "        if type(action) is str:\n",
    "            return action\n",
    "        code_action = ''\n",
    "        for i in action:\n",
    "            code_action += str(i)\n",
    "        return code_action\n",
    "\n",
    "    def decode_action(self, code_action):\n",
    "        return tuple([int(i) for i in code_action])\n",
    "\n",
    "    def encode_one_state(self, state):\n",
    "        code_state = ''\n",
    "        for i in state.flatten():\n",
    "            code_state += str(i) if i != -1 else '2'\n",
    "        return code_state\n",
    "\n",
    "    def generate_all_sym_states(self, state):\n",
    "        sym_states = [self.encode_one_state(state), self.encode_one_state(np.rot90(state, 1)),\n",
    "                      self.encode_one_state(np.rot90(state, 2)), self.encode_one_state(np.rot90(state, 3)),\n",
    "                      self.encode_one_state(state[::-1]), self.encode_one_state(state[:, ::-1]),\n",
    "                      self.encode_one_state(state.T), self.encode_one_state(np.rot90(state, 2).T)]\n",
    "        sym_states.sort()\n",
    "        return sym_states\n",
    "    \n",
    "    def encode_state_and_action(self, state, action):\n",
    "        empty_state = state * 0\n",
    "        empty_state[action] = 1\n",
    "        sym = [(self.encode_one_state(state), tuple(np.argwhere(empty_state == 1)[0])),\n",
    "               (self.encode_one_state(np.rot90(state, 1)), tuple(np.argwhere(np.rot90(empty_state, 1) == 1)[0])),\n",
    "               (self.encode_one_state(np.rot90(state, 2)), tuple(np.argwhere(np.rot90(empty_state, 2) == 1)[0])),\n",
    "               (self.encode_one_state(np.rot90(state, 3)), tuple(np.argwhere(np.rot90(empty_state, 3) == 1)[0])),\n",
    "               (self.encode_one_state(state[::-1]), tuple(np.argwhere(empty_state[::-1] == 1)[0])),\n",
    "               (self.encode_one_state(state[:, ::-1]), tuple(np.argwhere(empty_state[:, ::-1] == 1)[0])),\n",
    "               (self.encode_one_state(state.T), tuple(np.argwhere(empty_state.T == 1)[0])),\n",
    "               (self.encode_one_state(np.rot90(state, 2).T), tuple(np.argwhere(np.rot90(empty_state, 2).T == 1)[0]))]\n",
    "        sym.sort(key=lambda x: x[0])\n",
    "        code_state = sym[0][0]\n",
    "        tr_action = sym[0][1]\n",
    "        code_action = self.encode_action(tr_action)\n",
    "        return code_state, code_action\n",
    "\n",
    "    def encode_state(self, state):\n",
    "        sym_states = self.generate_all_sym_states(state)\n",
    "        return sym_states[0]\n",
    "    \n",
    "    def decode_one_state(self, code_state):\n",
    "        flat_list = [0 if elem == '0' else 1 if elem == '1' else -1 for elem in list(code_state)]\n",
    "        size = int(np.sqrt(len(code_state)))\n",
    "        state = np.reshape(flat_list, (size, size))\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "LhVXcQs8RlOD"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "class Game(gym.Env):\n",
    "    \n",
    "    def __init__(self, p1, p2, size=3, n_dim=2):\n",
    "        assert(type(n_dim) is int and n_dim >= 2), \"wrong n_dim\"\n",
    "        assert(type(size) is int and size >= 2), \"wrong size\"\n",
    "        self.n_dim = n_dim\n",
    "        self.size = size\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.turn = 1\n",
    "        self.board = np.zeros([size]*n_dim, dtype=int)\n",
    "        self.current_score = (0, 0)\n",
    "        self._max_episode_steps = 1000\n",
    "        super(Game, self).__init__()\n",
    "    \n",
    "    def simulate_games(self, n=100):\n",
    "        win_p1, win_p2, tot_even = 0, 0, 0\n",
    "        for _ in range(n // 2):\n",
    "            s1, s2 = self.play_a_game(verbose=False)\n",
    "            if s1 > s2:\n",
    "                win_p1 += 1\n",
    "            elif s2 > s1:\n",
    "                win_p2 += 1\n",
    "            else:\n",
    "                tot_even += 1\n",
    "        self.p1, self.p2 = self.p2, self.p1\n",
    "        for _ in range(n // 2):\n",
    "            s1, s2 = self.play_a_game(verbose=False)\n",
    "            if s1 > s2:\n",
    "                win_p2 += 1\n",
    "            elif s2 > s1:\n",
    "                win_p1 += 1\n",
    "            else:\n",
    "                tot_even += 1\n",
    "        self.p1, self.p2 = self.p2, self.p1\n",
    "        return win_p1, win_p2, tot_even\n",
    "    \n",
    "    def play_a_game(self, verbose=True):\n",
    "        self.reset()\n",
    "        if verbose:\n",
    "            self.render()\n",
    "        digits = '1234567890'\n",
    "        while not self.is_done():\n",
    "            if self.turn == 1:\n",
    "                player = self.p1\n",
    "            else:\n",
    "                player = self.p2\n",
    "            if isinstance(player, Agent):\n",
    "                coords = player.play_vs_opponent(self.board, self.turn)\n",
    "                if verbose:\n",
    "                    print('Agent plays :', coords, '\\n')\n",
    "            else:\n",
    "                coords_str = input('Coordinates of next move : ')\n",
    "                print()\n",
    "                coords = []\n",
    "                for c in coords_str:\n",
    "                    if c in digits:\n",
    "                        coords.append(int(c))\n",
    "                coords = tuple(coords)\n",
    "                while len(coords) != self.n_dim or max(coords) >= self.size or not self.is_available(coords):\n",
    "                    coords_str = input('Position not available, please try another one : ')\n",
    "                    coords = []\n",
    "                    for c in coords_str:\n",
    "                        if c in digits:\n",
    "                            coords.append(int(c))\n",
    "                    coords = tuple(coords)\n",
    "            self.step(coords)\n",
    "            if verbose:\n",
    "                self.render()\n",
    "        if verbose:\n",
    "            print('Game over. Score :', self.current_score)\n",
    "            if self.current_score[0] > self.current_score[1]:\n",
    "                print(self.p1, 'wins !')\n",
    "            elif self.current_score[1] > self.current_score[0]:\n",
    "                print(self.p2, 'wins !')\n",
    "            else:\n",
    "                print('Even score.')\n",
    "        return self.current_score\n",
    "    \n",
    "    def is_available(self, position):\n",
    "        return self.board[position] == 0\n",
    "    \n",
    "    def is_done(self):\n",
    "        if self.n_dim == 2:\n",
    "            return sum(self.current_score) != 0 or 0 not in self.board\n",
    "        return 0 not in self.board\n",
    "    \n",
    "    def reset(self):\n",
    "        self.turn = 1\n",
    "        self.current_score = (0, 0)\n",
    "        self.board *= 0\n",
    "        return self.board.copy()\n",
    "\n",
    "    def step(self, position):\n",
    "        self.board[position] = self.turn\n",
    "        score_p1, score_p2 = self.score()\n",
    "        score_p1_diff, score_p2_diff =  score_p1 - self.current_score[0], score_p2 - self.current_score[1]\n",
    "        # update only the score of the player that did the latest move\n",
    "        if self.turn == 1:\n",
    "            self.current_score = (score_p1, self.current_score[1])\n",
    "        else:\n",
    "            self.current_score = (self.current_score[0], score_p2)\n",
    "        reward = score_p1_diff if self.turn == 1 else score_p2_diff\n",
    "        self.turn *= -1\n",
    "        return self.board, reward, self.is_done(), None             \n",
    "    \n",
    "    def render(self):\n",
    "        visual_board = self.board.copy()\n",
    "        visual_board = np.where(visual_board == -1, 'O', visual_board)\n",
    "        visual_board = np.where(visual_board == '1', 'X', visual_board)\n",
    "        visual_board = np.where(visual_board == '0', '.', visual_board)\n",
    "        for icol in range(self.size):\n",
    "            for row in visual_board[icol, :]:\n",
    "                print(row, end=' ')\n",
    "            print()\n",
    "        print()\n",
    "    \n",
    "    def score(self):\n",
    "        score_p1 = 0\n",
    "        score_p2 = 0\n",
    "        \n",
    "        def slice_to_mask(L):\n",
    "            \"\"\"\n",
    "            Enables to use slicing operator like array[x, y, :, z] with choosing the position\n",
    "            of the symbol ':' (represented with a -1 instead). For example L can be equal to\n",
    "            [0, 0, -1, 0] if we want to access self.board[0, 0, :, 0]\n",
    "            \"\"\"\n",
    "            mask = np.zeros([self.size] * self.n_dim, dtype=bool)\n",
    "            dim = L.index(-1)\n",
    "            for tile in range(self.size):\n",
    "                L[dim] = tile\n",
    "                mask[tuple(L)] = True\n",
    "            return mask\n",
    "        \n",
    "        # vertical and horizontal axis\n",
    "        all_axis = []\n",
    "        for d in range(self.size ** self.n_dim):\n",
    "            all_axis.append([(d // self.size**k) % self.size for k in range(self.n_dim)[::-1]])\n",
    "            # example in 3D case with size 3 :\n",
    "            # all_axis = [ [i, j, k] for i = 0, 1, 2 for j = 0, 1, 2 for k = 0, 1, 2 ]\n",
    "        for d in range(self.n_dim):\n",
    "            d_axis = np.array(all_axis)\n",
    "            d_axis[:, d] = -1\n",
    "            d_axis = np.unique(d_axis, axis=0)\n",
    "            for axis in d_axis:\n",
    "                space_mask = slice_to_mask(list(axis))\n",
    "                in_game_axis = self.board[space_mask]\n",
    "                axis_value = in_game_axis.sum().item()\n",
    "                if axis_value == self.size:\n",
    "                    score_p1 += 1\n",
    "                elif axis_value == -self.size:\n",
    "                    score_p2 += 1\n",
    "        \n",
    "        # diagonal axis\n",
    "        diag = np.array([range(self.size)]).T\n",
    "        antidiag = np.array([range(self.size-1, -1, -1)]).T\n",
    "        poss_diag = np.array([diag, antidiag])\n",
    "        poss_index = list(range(self.size))\n",
    "        coords_to_check = set()\n",
    "        for dof in range(self.n_dim-2, -1, -1):\n",
    "            dof_fc = self.n_dim - dof\n",
    "            cpt = 0\n",
    "            for fc in subsets(poss_diag, dof_fc, repetition=True):\n",
    "                if cpt == int(dof_fc / 2) + 1:\n",
    "                    break\n",
    "                cpt += 1\n",
    "                frozen_comp = np.array(fc).reshape((dof_fc, self.size)).T\n",
    "                if dof > 0:\n",
    "                    for free_comp in subsets(poss_index, dof, repetition=True):\n",
    "                        free_comp_array = np.repeat(np.array([free_comp]), self.size, axis=0)\n",
    "                        coords = np.hstack((free_comp_array, frozen_comp))\n",
    "                        for perm in multiset_permutations(coords.T.tolist()):\n",
    "                            perm_coords = [list(i) for i in zip(*perm)]\n",
    "                            perm_coords.sort()\n",
    "                            coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                else:\n",
    "                    coords = frozen_comp\n",
    "                    for perm in multiset_permutations(coords.T.tolist()):\n",
    "                        perm_coords = [list(i) for i in zip(*perm)]\n",
    "                        perm_coords.sort()\n",
    "                        coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                        \n",
    "        for coords in coords_to_check:\n",
    "            total = 0\n",
    "            for tile in coords:\n",
    "                total += self.board[tile]\n",
    "            if abs(total) == self.size:\n",
    "                if total > 0:\n",
    "                    score_p1 += 1\n",
    "                else:\n",
    "                    score_p2 += 1\n",
    "                \n",
    "        return score_p1, score_p2\n",
    "    \n",
    "    def almost_align(self):\n",
    "        \n",
    "        def slice_to_mask(L):\n",
    "            \"\"\"\n",
    "            Enables to use slicing operator like array[x, y, :, z] with choosing the position\n",
    "            of the symbol ':' (represented with a -1 instead). For example L can be equal to\n",
    "            [0, 0, -1, 0] if we want to access self.board[0, 0, :, 0]\n",
    "            \"\"\"\n",
    "            mask = np.zeros([self.size] * self.n_dim, dtype=bool)\n",
    "            dim = L.index(-1)\n",
    "            for tile in range(self.size):\n",
    "                L[dim] = tile\n",
    "                mask[tuple(L)] = True\n",
    "            return mask\n",
    "        \n",
    "        score_p1 = 0\n",
    "        score_p2 = 0\n",
    "        # vertical and horizontal axis\n",
    "        all_axis = []\n",
    "        for d in range(self.size ** self.n_dim):\n",
    "            all_axis.append([(d // self.size**k) % self.size for k in range(self.n_dim)[::-1]])\n",
    "            # example in 3D case with size 3 :\n",
    "            # all_axis = [ [i, j, k] for i = 0, 1, 2 for j = 0, 1, 2 for k = 0, 1, 2 ]\n",
    "        for d in range(self.n_dim):\n",
    "            d_axis = np.array(all_axis)\n",
    "            d_axis[:, d] = -1\n",
    "            d_axis = np.unique(d_axis, axis=0)\n",
    "            for axis in d_axis:\n",
    "                space_mask = slice_to_mask(list(axis))\n",
    "                in_game_axis = self.board[space_mask]\n",
    "                axis_value = in_game_axis.sum().item()\n",
    "                if axis_value == self.size - 1:\n",
    "                    score_p1 += 1\n",
    "                elif axis_value == -self.size + 1:\n",
    "                    score_p2 += 1\n",
    "        \n",
    "        # diagonal axis\n",
    "        diag = np.array([range(self.size)]).T\n",
    "        antidiag = np.array([range(self.size-1, -1, -1)]).T\n",
    "        poss_diag = np.array([diag, antidiag])\n",
    "        poss_index = list(range(self.size))\n",
    "        coords_to_check = set()\n",
    "        for dof in range(self.n_dim-2, -1, -1):\n",
    "            dof_fc = self.n_dim - dof\n",
    "            cpt = 0\n",
    "            for fc in subsets(poss_diag, dof_fc, repetition=True):\n",
    "                if cpt == int(dof_fc / 2) + 1:\n",
    "                    break\n",
    "                cpt += 1\n",
    "                frozen_comp = np.array(fc).reshape((dof_fc, self.size)).T\n",
    "                if dof > 0:\n",
    "                    for free_comp in subsets(poss_index, dof, repetition=True):\n",
    "                        free_comp_array = np.repeat(np.array([free_comp]), self.size, axis=0)\n",
    "                        coords = np.hstack((free_comp_array, frozen_comp))\n",
    "                        for perm in multiset_permutations(coords.T.tolist()):\n",
    "                            perm_coords = [list(i) for i in zip(*perm)]\n",
    "                            perm_coords.sort()\n",
    "                            coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                else:\n",
    "                    coords = frozen_comp\n",
    "                    for perm in multiset_permutations(coords.T.tolist()):\n",
    "                        perm_coords = [list(i) for i in zip(*perm)]\n",
    "                        perm_coords.sort()\n",
    "                        coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                        \n",
    "        for coords in coords_to_check:\n",
    "            total = 0\n",
    "            for tile in coords:\n",
    "                total += self.board[tile]\n",
    "            if abs(total) == self.size - 1:\n",
    "                if total > 0:\n",
    "                    score_p1 += 1\n",
    "                else:\n",
    "                    score_p2 += 1\n",
    "                \n",
    "        return score_p1, score_p2\n",
    "\n",
    "    def board_position_to_tuple(self, pos):\n",
    "        resulting_position = []\n",
    "        for k in range(self.n_dim):\n",
    "            resulting_position.append(pos % self.size)\n",
    "            pos //= self.size\n",
    "        return tuple(resulting_position)\n",
    "\n",
    "    def board_position_to_index(self, pos):\n",
    "        #For dimension >= 3, we enumerate from higher, so this trick will work\n",
    "        res = 0\n",
    "        for i in range(len(pos)):\n",
    "            res += pos[i]*(self.size**(len(pos)-1 - i))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........CPU times: user 1min 9s, sys: 246 ms, total: 1min 9s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "agent = Agent(size=3)\n",
    "random_agent = Agent(size=3, policy=advanced_random_policy)\n",
    "game = Game(agent, random_agent, n_dim=2, size=3)\n",
    "sarsa(game, agent, random_policy, alpha=0.8, alpha_factor=0.999, gamma=0.9, epsilon=1.0, epsilon_factor=0.999,\n",
    "      r_win=5.0, r_lose=0.0, r_even=1.0, r_even2=1.5, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent won 346 times, lost 166 times and did 488 even games\n"
     ]
    }
   ],
   "source": [
    "win_p1, winp2, tot_even = game.simulate_games(1000)\n",
    "print('Agent won', win_p1, 'times, lost', winp2, 'times and did', tot_even, 'even games')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.p1, game.p2 = game.p2, game.p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . \n",
      ". . . \n",
      ". . . \n",
      "\n",
      "Agent plays : (2, 0) \n",
      "\n",
      ". . . \n",
      ". . . \n",
      "X . . \n",
      "\n",
      "Agent plays : (1, 1) \n",
      "\n",
      ". . . \n",
      ". O . \n",
      "X . . \n",
      "\n",
      "Agent plays : (2, 2) \n",
      "\n",
      ". . . \n",
      ". O . \n",
      "X . X \n",
      "\n",
      "Agent plays : (2, 1) \n",
      "\n",
      ". . . \n",
      ". O . \n",
      "X O X \n",
      "\n",
      "Agent plays : (0, 1) \n",
      "\n",
      ". X . \n",
      ". O . \n",
      "X O X \n",
      "\n",
      "Agent plays : (0, 0) \n",
      "\n",
      "O X . \n",
      ". O . \n",
      "X O X \n",
      "\n",
      "Agent plays : (1, 2) \n",
      "\n",
      "O X . \n",
      ". O X \n",
      "X O X \n",
      "\n",
      "Agent plays : (0, 2) \n",
      "\n",
      "O X O \n",
      ". O X \n",
      "X O X \n",
      "\n",
      "Agent plays : (1, 0) \n",
      "\n",
      "O X O \n",
      "X O X \n",
      "X O X \n",
      "\n",
      "Game over. Score : (0, 0)\n",
      "Even score.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.play_a_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "29841 0 26246\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "grid_size = 3\n",
    "n_dim = 2\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, size=3, n_dim=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(size**n_dim,size**n_dim*3)\n",
    "        self.fc2 = torch.nn.Linear(size**n_dim*3, size**n_dim*3)\n",
    "        self.fc3 = torch.nn.Linear(size**n_dim*3, size**n_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.view(-1).type(torch.long)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.nn.ReLU()(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.nn.Softmax(dim=1)(x)\n",
    "        return x\n",
    "\n",
    "agent1 = Model(3, 3) # randomly generate weight\n",
    "agent2 = Model(3, 3)\n",
    "\n",
    "game = Game(None, None, 3, 3)\n",
    "#print(\"transform \", game.board_position_to_index((2, 1, 1)))\n",
    "\n",
    "def get_best_possible(board, probs):\n",
    "    probs = [(probs[i], i) for i in range(probs.shape[0])]\n",
    "    probs = sorted(probs, reverse=True)\n",
    "    for j in probs:\n",
    "        position = j[1]\n",
    "        resulting_position = game.board_position_to_tuple(position)\n",
    "        if board[resulting_position] == 0:\n",
    "            return (position, resulting_position)\n",
    "    return None\n",
    "\n",
    "def make_move(game, model):\n",
    "    prev_state = np.reshape(game.board, (1, -1))\n",
    "    state = torch.tensor(game.board).view(1, -1).type(torch.float)\n",
    "    #print(state.shape)\n",
    "    result = model(state).detach().numpy()[0]\n",
    "    best_move, best_move_tuple = get_best_possible(game.board, result)\n",
    "    new_state, reward, is_done, _ = game.step(best_move_tuple)\n",
    "    return prev_state, reward, best_move, new_state\n",
    "\n",
    "\n",
    "\n",
    "def loss(states, actions, rewards, model):\n",
    "    predicts = torch.log(model(states))\n",
    "    losses = rewards * predicts[np.arange(len(actions)), actions]\n",
    "    loss = -losses.mean()\n",
    "    return loss\n",
    "\n",
    "def train_network(model, game, num_of_iterations):\n",
    "    optimizer = torch.optim.SGD(agent1.parameters(), lr=0.01)\n",
    "    loss_values = []\n",
    "    game_records = []\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    loses = 0\n",
    "    for i in range(num_of_iterations):\n",
    "        game.reset()\n",
    "        game_record = []\n",
    "        while not game.is_done():\n",
    "            result = make_move(game, agent1)\n",
    "            game_record.append(result)\n",
    "            if game.is_done():\n",
    "                break\n",
    "            #result = make_move(game, agent2)\n",
    "            prev_state = np.reshape(game.board, (1, -1))\n",
    "            move = random_policy(game.board, '')\n",
    "            new_state, reward, is_done, _ = game.step(move)\n",
    "            game_record.append((prev_state, reward, game.board_position_to_index((move))))\n",
    "        #print(game.current_score)\n",
    "\n",
    "        wins += game.current_score[0]\n",
    "        loses += game.current_score[1]\n",
    "        if game.current_score == (0,0):\n",
    "            draws += 1\n",
    "        scaling_coeff = 0.8\n",
    "        if game.current_score[0] == 1:\n",
    "            for j in range(len(game_record)-2, 0, -1):\n",
    "                record = list(game_record[j])\n",
    "                record[1] += game_record[j+1][1] * scaling_coeff\n",
    "                game_record[j] = tuple(record)\n",
    "        #get a look on draw and lose reward\n",
    "        for j in game_record:\n",
    "            game_records.append(j)\n",
    "        if i % 100 == 0 and i != 0:\n",
    "            optimizer.zero_grad()\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            for j in game_records:\n",
    "                #print(j[1][0])\n",
    "                states.append(j[0][0]) # to check why\n",
    "                actions.append(j[2])\n",
    "                rewards.append(j[1])\n",
    "            states = torch.tensor(states).type(torch.float)\n",
    "            actions = torch.tensor(actions)\n",
    "            rewards = torch.tensor(rewards)\n",
    "            l = loss(states, actions, rewards, agent1)\n",
    "            loss_values.append(l.detach().numpy())\n",
    "\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            game_records = []\n",
    "            #print(actions)\n",
    "            #print(rewards)\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    return model, loss_values, wins, draws, loses\n",
    "\n",
    "\n",
    "agent1, loss_values, wins, draws, loses = train_network(agent1, game, 5000)\n",
    "print(wins, draws, loses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.287927\n",
      "[0 1 2 3 4 5 6 7 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn\n",
    "print(loss_values[0])\n",
    "#loss_values = [i[0] for i in loss_values]\n",
    "loss = []\n",
    "for i in loss_values:\n",
    "    loss.append(float(i))\n",
    "print(np.arange(len(loss_values)))\n",
    "seaborn.lineplot(np.arange(len(loss_values)), loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "num_of_iteration = 10000\n",
    "wins = 0\n",
    "draws = 0\n",
    "loses = 0\n",
    "for i in range(num_of_iteration):\n",
    "    game.reset()\n",
    "    while not game.is_done():\n",
    "        make_move(game, agent1)\n",
    "        if game.is_done():\n",
    "            break\n",
    "        move = random_policy(game.board, '')\n",
    "        new_state, reward, is_done, _ = game.step(move)\n",
    "\n",
    "    wins += game.current_score[0]\n",
    "    loses += game.current_score[1]\n",
    "    if game.current_score == (0,0):\n",
    "        draws += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RL_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}