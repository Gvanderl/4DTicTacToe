{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "meNNqqqlRlN4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from sympy.utilities.iterables import subsets\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "from scipy.special import comb\n",
    "import pandas as pd\n",
    "import gym\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "\n",
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_almost_align(board, s):\n",
    "    size = board.shape[0]\n",
    "    for irow in range(size):\n",
    "        if board[irow, :].sum() == s * (size - 1):\n",
    "            i_empty = np.where(board[irow, :] == 0)[0].item()\n",
    "            return (irow, i_empty)\n",
    "    for icol in range(size):\n",
    "        if board[:, icol].sum() == s * (size - 1):\n",
    "            i_empty = np.where(board[:, icol] == 0)[0].item()\n",
    "            return (i_empty, icol)\n",
    "    if np.diag(board).sum() == s * (size - 1):\n",
    "        i_empty = np.where(np.diag(board) == 0)[0].item()\n",
    "        return (i_empty, i_empty)\n",
    "    if np.diag(np.rot90(board)).sum() == s * (size - 1):\n",
    "        for i in range(size):\n",
    "            if board[i, size - 1 - i] == 0:\n",
    "                return (i, size - 1 - i)\n",
    "    return None\n",
    "\n",
    "def random_policy(board, symbol):\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    pos = np.random.randint(available_pos.shape[0])\n",
    "    return tuple(available_pos[np.random.randint(available_pos.shape[0])])\n",
    "\n",
    "def linear_policy(board, symbol):\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    return tuple(available_pos[0])\n",
    "\n",
    "def advanced_static_policy(board, symbol):\n",
    "    coords = check_almost_align(board, symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    coords = check_almost_align(board, -symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    return tuple(available_pos[0])\n",
    "\n",
    "def advanced_random_policy(board, symbol):\n",
    "    coords = check_almost_align(board, symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    coords = check_almost_align(board, -symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    return tuple(available_pos[np.random.randint(available_pos.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_Wavup5eRlOB"
   },
   "outputs": [],
   "source": [
    "def sarsa(game, agent, opponent_policy, alpha, alpha_factor, gamma, epsilon, epsilon_factor, \\\n",
    "          r_win, r_lose, r_even, r_even2, num_episodes):\n",
    "    for episode_index in range(num_episodes):\n",
    "        if episode_index % 1000 == 0:\n",
    "            print('.', end='')\n",
    "        alpha *= alpha_factor\n",
    "        epsilon *= epsilon_factor\n",
    "        state = game.reset()\n",
    "        if episode_index%2 == 1:\n",
    "            action = opponent_policy(state, game.turn)\n",
    "            state, _, _, _ = game.step(action)\n",
    "        action = agent.epsilon_greedy_policy(state, epsilon)\n",
    "        state_history = [state.copy()]\n",
    "        action_history = [action]\n",
    "        while True:\n",
    "            ################### The agent ignores this happens UNLESS it ends the game\n",
    "            intermediate_state, reward, done, _ = game.step(action)\n",
    "            if done:\n",
    "                if reward > 0:\n",
    "                    final_reward = r_win\n",
    "                else:\n",
    "                    final_reward = r_even\n",
    "                break\n",
    "            intermediate_action = opponent_policy(intermediate_state, game.turn)\n",
    "            ################### ------------------------------------------------\n",
    "            state, reward, done, _ = game.step(intermediate_action)\n",
    "            if done:\n",
    "                if reward > 0:\n",
    "                    reward = r_lose\n",
    "                else:\n",
    "                    reward = r_even2\n",
    "                break\n",
    "            action = agent.epsilon_greedy_policy(state, epsilon)\n",
    "            state_history.append(state.copy())\n",
    "            action_history.append(action)\n",
    "            \n",
    "        agent.update_Qtable(state_history, action_history, reward, alpha, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yi4p1W_wRlOC"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, size=3, policy=None):\n",
    "        self.q_array = pd.DataFrame(columns=[str(i)+str(j) for i in range(size) for j in range(size)])\n",
    "        self.policy = policy\n",
    "        self.identity = {}\n",
    "        self.convert_rot90_1 = {}\n",
    "        self.convert_rot90_2 = {}\n",
    "        self.convert_rot90_3 = {}\n",
    "        self.convert_horizontal_axis = {}\n",
    "        self.convert_vertical_axis = {}\n",
    "        self.convert_diag = {}\n",
    "        self.convert_antidiag = {}\n",
    "        empty = np.zeros([size]*2, dtype=int)\n",
    "        \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = empty\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.identity[str(i)+str(j)] = coords_transform\n",
    "        \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = np.rot90(empty, 1)\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_rot90_1[str(i)+str(j)] = coords_transform\n",
    "        \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = np.rot90(empty, 2)\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_rot90_2[str(i)+str(j)] = coords_transform\n",
    "                \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = np.rot90(empty, 3)\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_rot90_3[str(i)+str(j)] = coords_transform\n",
    "                \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = empty[::-1]\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_horizontal_axis[str(i)+str(j)] = coords_transform\n",
    "        \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = empty[:, ::-1]\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_vertical_axis[str(i)+str(j)] = coords_transform\n",
    "        \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = empty.T\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_diag[str(i)+str(j)] = coords_transform\n",
    "                \n",
    "        for i in range(empty.shape[0]):\n",
    "            for j in range(empty.shape[0]):\n",
    "                empty *= 0\n",
    "                empty[i, j] = 1\n",
    "                tr_empty = np.rot90(empty, 2).T\n",
    "                coords_transform = tuple(np.argwhere(tr_empty == 1)[0])\n",
    "                self.convert_antidiag[str(i)+str(j)] = coords_transform\n",
    "                \n",
    "    \n",
    "    def update_Qtable(self, state_history, action_history, reward, alpha, gamma): \n",
    "        code_last_state, code_last_action = self.encode_state_and_action(state_history[-1], action_history[-1])\n",
    "        try:\n",
    "            self.q_array.loc[code_last_state]\n",
    "        except:\n",
    "            self.q_array.loc[code_last_state] = 0.5\n",
    "        self.q_array.loc[code_last_state, code_last_action] = reward\n",
    "        \n",
    "        for i in range(len(state_history)-2, -1, -1):\n",
    "            state = state_history[i]\n",
    "            new_state = state_history[i+1]\n",
    "            action = action_history[i]\n",
    "            new_action = action_history[i+1]\n",
    "            code_state, code_action = self.encode_state_and_action(state, action)\n",
    "            try:\n",
    "                self.q_array.loc[code_state]\n",
    "            except:\n",
    "                self.q_array.loc[code_state] = 0.5\n",
    "            code_new_state, code_new_action = self.encode_state_and_action(new_state, new_action)\n",
    "            self.q_array.loc[code_state, code_action] = (1 - alpha) * self.q_array.loc[code_state, code_action] \\\n",
    "                                                        + alpha * gamma * self.q_array.loc[code_new_state, code_new_action]\n",
    "    \n",
    "    def play_vs_opponent(self, state, symbol):\n",
    "        if self.policy is not None:\n",
    "            return self.policy(state, symbol)\n",
    "        state_code = self.encode_state(state)\n",
    "        try:\n",
    "            self.q_array.loc[state_code]\n",
    "        except:\n",
    "            legal_moves = np.argwhere(state == 0)\n",
    "            return tuple(legal_moves[np.random.randint(legal_moves.shape[0])])\n",
    "        ###### Find best move\n",
    "        potential_actions = self.q_array.loc[state_code]\n",
    "        reference_state = self.decode_one_state(state_code)\n",
    "        free_spots_scores = []\n",
    "        \n",
    "        if np.prod(state == reference_state):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.identity[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == np.rot90(reference_state, 1)):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_rot90_1[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == np.rot90(reference_state, 2)):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_rot90_2[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == np.rot90(reference_state, 3)):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_rot90_3[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == reference_state[::-1]):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_horizontal_axis[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == reference_state[:, ::-1]):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_vertical_axis[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == reference_state.T):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_diag[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == np.rot90(reference_state, 2).T):\n",
    "            for a in potential_actions.index:\n",
    "                tr_coords = self.convert_antidiag[a]\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        else:\n",
    "            print('Error in play_vs_opponent in Agent !!!')\n",
    "        max_reward = max(free_spots_scores, key=lambda x: x[1])[1]\n",
    "        best_actions = [act for act, rew in free_spots_scores if rew == max_reward]\n",
    "        return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "    def greedy_policy(self, state):\n",
    "        \"\"\"\n",
    "        Return the next action as a tuple\n",
    "        \"\"\"\n",
    "        code_state = self.encode_state(state)\n",
    "        try:\n",
    "            self.q_array.loc[code_state]\n",
    "        except:\n",
    "            self.q_array.loc[code_state] = 0.5\n",
    "        reference_state = self.decode_one_state(code_state)\n",
    "        legal_actions = []\n",
    "        actions = self.q_array.loc[code_state]\n",
    "            \n",
    "        if np.prod(state == reference_state):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.identity[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == np.rot90(reference_state, 1)):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_rot90_1[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == np.rot90(reference_state, 2)):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_rot90_2[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == np.rot90(reference_state, 3)):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_rot90_3[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == reference_state[::-1]):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_horizontal_axis[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == reference_state[:, ::-1]):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_vertical_axis[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == reference_state.T):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_diag[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == np.rot90(reference_state, 2).T):\n",
    "            for act in actions.index:\n",
    "                coords_transform = self.convert_antidiag[act]\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        print('\\nError in greedy policy !!!\\n')\n",
    "        print(state, '\\n')\n",
    "        print(reference_state, '\\n')\n",
    "        print(reference_state[::-1][::-1].T)\n",
    "\n",
    "    def epsilon_greedy_policy(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Return the next action as a tuple\n",
    "        \"\"\"\n",
    "        if self.q_array.empty or np.random.rand() < epsilon:\n",
    "            legal_moves = np.argwhere(state == 0)\n",
    "            size = legal_moves.shape[0]\n",
    "            random_idx = np.random.randint(size)\n",
    "            action = tuple(legal_moves[random_idx])\n",
    "        else:\n",
    "            action = self.greedy_policy(state)\n",
    "        return action\n",
    "    \n",
    "    def encode_action(self, action):\n",
    "        if type(action) is str:\n",
    "            return action\n",
    "        code_action = ''\n",
    "        for i in action:\n",
    "            code_action += str(i)\n",
    "        return code_action\n",
    "\n",
    "    def decode_action(self, code_action):\n",
    "        return tuple([int(i) for i in code_action])\n",
    "\n",
    "    def encode_one_state(self, state):\n",
    "        code_state = ''\n",
    "        for i in state.flatten():\n",
    "            code_state += str(i) if i != -1 else '2'\n",
    "        return code_state\n",
    "\n",
    "    def generate_all_sym_states(self, state):\n",
    "        sym_states = [self.encode_one_state(state), self.encode_one_state(np.rot90(state, 1)), \\\n",
    "                      self.encode_one_state(np.rot90(state, 2)), self.encode_one_state(np.rot90(state, 3)), \\\n",
    "                      self.encode_one_state(state[::-1]), self.encode_one_state(state[:, ::-1]), \\\n",
    "                      self.encode_one_state(state.T), self.encode_one_state(np.rot90(state, 2).T)]\n",
    "        sym_states.sort()\n",
    "        return sym_states\n",
    "    \n",
    "    def encode_state_and_action(self, state, action):\n",
    "        empty_state = state * 0\n",
    "        empty_state[action] = 1\n",
    "        sym = [(self.encode_one_state(state), tuple(np.argwhere(empty_state == 1)[0])), \\\n",
    "               (self.encode_one_state(np.rot90(state, 1)), tuple(np.argwhere(np.rot90(empty_state, 1) == 1)[0])), \\\n",
    "               (self.encode_one_state(np.rot90(state, 2)), tuple(np.argwhere(np.rot90(empty_state, 2) == 1)[0])), \\\n",
    "               (self.encode_one_state(np.rot90(state, 3)), tuple(np.argwhere(np.rot90(empty_state, 3) == 1)[0])), \\\n",
    "               (self.encode_one_state(state[::-1]), tuple(np.argwhere(empty_state[::-1] == 1)[0])), \\\n",
    "               (self.encode_one_state(state[:, ::-1]), tuple(np.argwhere(empty_state[:, ::-1] == 1)[0])), \\\n",
    "               (self.encode_one_state(state.T), tuple(np.argwhere(empty_state.T == 1)[0])), \\\n",
    "               (self.encode_one_state(np.rot90(state, 2).T), tuple(np.argwhere(np.rot90(empty_state, 2).T == 1)[0]))]\n",
    "        sym.sort(key=lambda x: x[0])\n",
    "        code_state = sym[0][0]\n",
    "        tr_action = sym[0][1]\n",
    "        code_action = self.encode_action(tr_action)\n",
    "        return code_state, code_action\n",
    "\n",
    "    def encode_state(self, state):\n",
    "        sym_states = self.generate_all_sym_states(state)\n",
    "        return sym_states[0]\n",
    "    \n",
    "    def decode_one_state(self, code_state):\n",
    "        flat_list = [0 if elem == '0' else 1 if elem == '1' else -1 for elem in list(code_state)]\n",
    "        size = int(np.sqrt(len(code_state)))\n",
    "        state = np.reshape(flat_list, (size, size))\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "LhVXcQs8RlOD"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "class Game(gym.Env):\n",
    "    \n",
    "    def __init__(self, p1, p2, size=3, n_dim=2):\n",
    "        assert(type(n_dim) is int and n_dim >= 2), \"wrong n_dim\"\n",
    "        assert(type(size) is int and size >= 2), \"wrong size\"\n",
    "        self.n_dim = n_dim\n",
    "        self.size = size\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.turn = 1\n",
    "        self.board = np.zeros([size]*n_dim, dtype=int)\n",
    "        self.current_score = (0, 0)\n",
    "        self._max_episode_steps = 1000\n",
    "        super(Game, self).__init__()\n",
    "    \n",
    "    def simulate_games(self, n=100):\n",
    "        win_p1, win_p2, tot_even = 0, 0, 0\n",
    "        for _ in range(n // 2):\n",
    "            s1, s2 = self.play_a_game(verbose=False)\n",
    "            if s1 > s2:\n",
    "                win_p1 += 1\n",
    "            elif s2 > s1:\n",
    "                win_p2 += 1\n",
    "            else:\n",
    "                tot_even += 1\n",
    "        self.p1, self.p2 = self.p2, self.p1\n",
    "        for _ in range(n // 2):\n",
    "            s1, s2 = self.play_a_game(verbose=False)\n",
    "            if s1 > s2:\n",
    "                win_p2 += 1\n",
    "            elif s2 > s1:\n",
    "                win_p1 += 1\n",
    "            else:\n",
    "                tot_even += 1\n",
    "        self.p1, self.p2 = self.p2, self.p1\n",
    "        return win_p1, win_p2, tot_even\n",
    "    \n",
    "    def play_a_game(self, verbose=True):\n",
    "        self.reset()\n",
    "        if verbose:\n",
    "            self.render()\n",
    "        digits = '1234567890'\n",
    "        while not self.is_done():\n",
    "            if self.turn == 1:\n",
    "                player = self.p1\n",
    "            else:\n",
    "                player = self.p2\n",
    "            if isinstance(player, Agent):\n",
    "                coords = player.play_vs_opponent(self.board, self.turn)\n",
    "                if verbose:\n",
    "                    print('Agent plays :', coords, '\\n')\n",
    "            else:\n",
    "                coords_str = input('Coordinates of next move : ')\n",
    "                print()\n",
    "                coords = []\n",
    "                for c in coords_str:\n",
    "                    if c in digits:\n",
    "                        coords.append(int(c))\n",
    "                coords = tuple(coords)\n",
    "                while len(coords) != self.n_dim or max(coords) >= self.size or not self.is_available(coords):\n",
    "                    coords_str = input('Position not available, please try another one : ')\n",
    "                    coords = []\n",
    "                    for c in coords_str:\n",
    "                        if c in digits:\n",
    "                            coords.append(int(c))\n",
    "                    coords = tuple(coords)\n",
    "            self.step(coords)\n",
    "            if verbose:\n",
    "                self.render()\n",
    "        if verbose:\n",
    "            print('Game over. Score :', self.current_score)\n",
    "            if self.current_score[0] > self.current_score[1]:\n",
    "                print(self.p1, 'wins !')\n",
    "            elif self.current_score[1] > self.current_score[0]:\n",
    "                print(self.p2, 'wins !')\n",
    "            else:\n",
    "                print('Even score.')\n",
    "        return self.current_score\n",
    "    \n",
    "    def is_available(self, position):\n",
    "        return self.board[position] == 0\n",
    "    \n",
    "    def is_done(self):\n",
    "        if self.n_dim == 2:\n",
    "            return sum(self.current_score) != 0 or 0 not in self.board\n",
    "        return 0 not in self.board\n",
    "    \n",
    "    def reset(self):\n",
    "        self.turn = 1\n",
    "        self.current_score = (0, 0)\n",
    "        self.board *= 0\n",
    "        return self.board.copy()\n",
    "\n",
    "    def step(self, position):\n",
    "        self.board[position] = self.turn\n",
    "        score_p1, score_p2 = self.score()\n",
    "        score_p1_diff, score_p2_diff =  score_p1 - self.current_score[0], score_p2 - self.current_score[1]\n",
    "        # update only the score of the player that did the latest move\n",
    "        if self.turn == 1:\n",
    "            self.current_score = (score_p1, self.current_score[1])\n",
    "        else:\n",
    "            self.current_score = (self.current_score[0], score_p2)\n",
    "        reward = score_p1_diff if self.turn == 1 else score_p2_diff\n",
    "        self.turn *= -1\n",
    "        return self.board, reward, self.is_done(), None             \n",
    "    \n",
    "    def render(self):\n",
    "        visual_board = self.board.copy()\n",
    "        visual_board = np.where(visual_board == -1, 'O', visual_board)\n",
    "        visual_board = np.where(visual_board == '1', 'X', visual_board)\n",
    "        visual_board = np.where(visual_board == '0', '.', visual_board)\n",
    "        for icol in range(self.size):\n",
    "            for row in visual_board[icol, :]:\n",
    "                print(row, end=' ')\n",
    "            print()\n",
    "        print()\n",
    "    \n",
    "    def score(self):\n",
    "        score_p1 = 0\n",
    "        score_p2 = 0\n",
    "        \n",
    "        def slice_to_mask(L):\n",
    "            \"\"\"\n",
    "            Enables to use slicing operator like array[x, y, :, z] with choosing the position\n",
    "            of the symbol ':' (represented with a -1 instead). For example L can be equal to\n",
    "            [0, 0, -1, 0] if we want to access self.board[0, 0, :, 0]\n",
    "            \"\"\"\n",
    "            mask = np.zeros([self.size] * self.n_dim, dtype=bool)\n",
    "            dim = L.index(-1)\n",
    "            for tile in range(self.size):\n",
    "                L[dim] = tile\n",
    "                mask[tuple(L)] = True\n",
    "            return mask\n",
    "        \n",
    "        # vertical and horizontal axis\n",
    "        all_axis = []\n",
    "        for d in range(self.size ** self.n_dim):\n",
    "            all_axis.append([(d // self.size**k) % self.size for k in range(self.n_dim)[::-1]])\n",
    "            # example in 3D case with size 3 :\n",
    "            # all_axis = [ [i, j, k] for i = 0, 1, 2 for j = 0, 1, 2 for k = 0, 1, 2 ]\n",
    "        for d in range(self.n_dim):\n",
    "            d_axis = np.array(all_axis)\n",
    "            d_axis[:, d] = -1\n",
    "            d_axis = np.unique(d_axis, axis=0)\n",
    "            for axis in d_axis:\n",
    "                space_mask = slice_to_mask(list(axis))\n",
    "                in_game_axis = self.board[space_mask]\n",
    "                axis_value = in_game_axis.sum().item()\n",
    "                if axis_value == self.size:\n",
    "                    score_p1 += 1\n",
    "                elif axis_value == -self.size:\n",
    "                    score_p2 += 1\n",
    "        \n",
    "        # diagonal axis\n",
    "        diag = np.array([range(self.size)]).T\n",
    "        antidiag = np.array([range(self.size-1, -1, -1)]).T\n",
    "        poss_diag = np.array([diag, antidiag])\n",
    "        poss_index = list(range(self.size))\n",
    "        coords_to_check = set()\n",
    "        for dof in range(self.n_dim-2, -1, -1):\n",
    "            dof_fc = self.n_dim - dof\n",
    "            cpt = 0\n",
    "            for fc in subsets(poss_diag, dof_fc, repetition=True):\n",
    "                if cpt == int(dof_fc / 2) + 1:\n",
    "                    break\n",
    "                cpt += 1\n",
    "                frozen_comp = np.array(fc).reshape((dof_fc, self.size)).T\n",
    "                if dof > 0:\n",
    "                    for free_comp in subsets(poss_index, dof, repetition=True):\n",
    "                        free_comp_array = np.repeat(np.array([free_comp]), self.size, axis=0)\n",
    "                        coords = np.hstack((free_comp_array, frozen_comp))\n",
    "                        for perm in multiset_permutations(coords.T.tolist()):\n",
    "                            perm_coords = [list(i) for i in zip(*perm)]\n",
    "                            perm_coords.sort()\n",
    "                            coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                else:\n",
    "                    coords = frozen_comp\n",
    "                    for perm in multiset_permutations(coords.T.tolist()):\n",
    "                        perm_coords = [list(i) for i in zip(*perm)]\n",
    "                        perm_coords.sort()\n",
    "                        coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                        \n",
    "        for coords in coords_to_check:\n",
    "            total = 0\n",
    "            for tile in coords:\n",
    "                total += self.board[tile]\n",
    "            if abs(total) == self.size:\n",
    "                if total > 0:\n",
    "                    score_p1 += 1\n",
    "                else:\n",
    "                    score_p2 += 1\n",
    "                \n",
    "        return score_p1, score_p2\n",
    "    \n",
    "    def almost_align(self):\n",
    "        \n",
    "        def slice_to_mask(L):\n",
    "            \"\"\"\n",
    "            Enables to use slicing operator like array[x, y, :, z] with choosing the position\n",
    "            of the symbol ':' (represented with a -1 instead). For example L can be equal to\n",
    "            [0, 0, -1, 0] if we want to access self.board[0, 0, :, 0]\n",
    "            \"\"\"\n",
    "            mask = np.zeros([self.size] * self.n_dim, dtype=bool)\n",
    "            dim = L.index(-1)\n",
    "            for tile in range(self.size):\n",
    "                L[dim] = tile\n",
    "                mask[tuple(L)] = True\n",
    "            return mask\n",
    "        \n",
    "        score_p1 = 0\n",
    "        score_p2 = 0\n",
    "        # vertical and horizontal axis\n",
    "        all_axis = []\n",
    "        for d in range(self.size ** self.n_dim):\n",
    "            all_axis.append([(d // self.size**k) % self.size for k in range(self.n_dim)[::-1]])\n",
    "            # example in 3D case with size 3 :\n",
    "            # all_axis = [ [i, j, k] for i = 0, 1, 2 for j = 0, 1, 2 for k = 0, 1, 2 ]\n",
    "        for d in range(self.n_dim):\n",
    "            d_axis = np.array(all_axis)\n",
    "            d_axis[:, d] = -1\n",
    "            d_axis = np.unique(d_axis, axis=0)\n",
    "            for axis in d_axis:\n",
    "                space_mask = slice_to_mask(list(axis))\n",
    "                in_game_axis = self.board[space_mask]\n",
    "                axis_value = in_game_axis.sum().item()\n",
    "                if axis_value == self.size - 1:\n",
    "                    score_p1 += 1\n",
    "                elif axis_value == -self.size + 1:\n",
    "                    score_p2 += 1\n",
    "        \n",
    "        # diagonal axis\n",
    "        diag = np.array([range(self.size)]).T\n",
    "        antidiag = np.array([range(self.size-1, -1, -1)]).T\n",
    "        poss_diag = np.array([diag, antidiag])\n",
    "        poss_index = list(range(self.size))\n",
    "        coords_to_check = set()\n",
    "        for dof in range(self.n_dim-2, -1, -1):\n",
    "            dof_fc = self.n_dim - dof\n",
    "            cpt = 0\n",
    "            for fc in subsets(poss_diag, dof_fc, repetition=True):\n",
    "                if cpt == int(dof_fc / 2) + 1:\n",
    "                    break\n",
    "                cpt += 1\n",
    "                frozen_comp = np.array(fc).reshape((dof_fc, self.size)).T\n",
    "                if dof > 0:\n",
    "                    for free_comp in subsets(poss_index, dof, repetition=True):\n",
    "                        free_comp_array = np.repeat(np.array([free_comp]), self.size, axis=0)\n",
    "                        coords = np.hstack((free_comp_array, frozen_comp))\n",
    "                        for perm in multiset_permutations(coords.T.tolist()):\n",
    "                            perm_coords = [list(i) for i in zip(*perm)]\n",
    "                            perm_coords.sort()\n",
    "                            coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                else:\n",
    "                    coords = frozen_comp\n",
    "                    for perm in multiset_permutations(coords.T.tolist()):\n",
    "                        perm_coords = [list(i) for i in zip(*perm)]\n",
    "                        perm_coords.sort()\n",
    "                        coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                        \n",
    "        for coords in coords_to_check:\n",
    "            total = 0\n",
    "            for tile in coords:\n",
    "                total += self.board[tile]\n",
    "            if abs(total) == self.size - 1:\n",
    "                if total > 0:\n",
    "                    score_p1 += 1\n",
    "                else:\n",
    "                    score_p2 += 1\n",
    "                \n",
    "        return score_p1, score_p2\n",
    "\n",
    "    def board_position_to_tuple(self, pos):\n",
    "        resulting_position = []\n",
    "        for k in range(self.n_dim):\n",
    "            resulting_position.append(pos % self.size)\n",
    "            pos //= self.size\n",
    "        return tuple(resulting_position)\n",
    "\n",
    "    def board_position_to_index(self, pos):\n",
    "        #For dimension >= 3, we enumerate from higher, so this trick will work\n",
    "        res = 0\n",
    "        for i in range(len(pos)):\n",
    "            res += pos[i]*(self.size**(len(pos)-1 - i))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-fab77dc11505>\u001B[0m in \u001B[0;36msarsa\u001B[0;34m(game, agent, opponent_policy, alpha, alpha_factor, gamma, epsilon, epsilon_factor, r_win, r_lose, r_even, r_even2, num_episodes)\u001B[0m\n\u001B[1;32m     35\u001B[0m             \u001B[0maction_history\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m         \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate_Qtable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate_history\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction_history\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgamma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     38\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-6-3ddbfb7c93f8>\u001B[0m in \u001B[0;36mupdate_Qtable\u001B[0;34m(self, state_history, action_history, reward, alpha, gamma)\u001B[0m\n\u001B[1;32m     97\u001B[0m             \u001B[0;32mexcept\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     98\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mq_array\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcode_state\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.5\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 99\u001B[0;31m             \u001B[0mcode_new_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcode_new_action\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencode_state_and_action\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew_action\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    100\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mq_array\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcode_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcode_action\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mq_array\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcode_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcode_action\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m                                                         \u001B[0;34m+\u001B[0m \u001B[0malpha\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mgamma\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mq_array\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcode_new_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcode_new_action\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-6-3ddbfb7c93f8>\u001B[0m in \u001B[0;36mencode_state_and_action\u001B[0;34m(self, state, action)\u001B[0m\n\u001B[1;32m    301\u001B[0m                \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencode_one_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrot90\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margwhere\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrot90\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mempty_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    302\u001B[0m                \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencode_one_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrot90\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margwhere\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrot90\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mempty_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 303\u001B[0;31m                \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencode_one_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrot90\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margwhere\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrot90\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mempty_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    304\u001B[0m                \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencode_one_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margwhere\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mempty_state\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    305\u001B[0m                \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencode_one_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margwhere\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mempty_state\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mrot90\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.9/site-packages/numpy/lib/function_base.py\u001B[0m in \u001B[0;36mrot90\u001B[0;34m(m, k, axes)\u001B[0m\n\u001B[1;32m    133\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mflip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 135\u001B[0;31m     \u001B[0maxes_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    136\u001B[0m     (axes_list[axes[0]], axes_list[axes[1]]) = (axes_list[axes[1]],\n\u001B[1;32m    137\u001B[0m                                                 axes_list[axes[0]])\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "agent = Agent(size=3)\n",
    "random_agent = Agent(size=3, policy=advanced_random_policy)\n",
    "game = Game(agent, random_agent, n_dim=2, size=3)\n",
    "sarsa(game, agent, random_policy, alpha=0.8, alpha_factor=0.999, gamma=0.9, epsilon=1.0, epsilon_factor=0.999, \\\n",
    "      r_win=5.0, r_lose=0.0, r_even=1.0, r_even2=1.5, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent won 215 times, lost 121 times and did 664 even games\n"
     ]
    }
   ],
   "source": [
    "win_p1, winp2, tot_even = game.simulate_games(1000)\n",
    "print('Agent won', win_p1, 'times, lost', winp2, 'times and did', tot_even, 'even games')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.p1, game.p2 = game.p2, game.p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . \n",
      ". . . \n",
      ". . . \n",
      "\n",
      "Agent plays : (2, 0) \n",
      "\n",
      ". . . \n",
      ". . . \n",
      "X . . \n",
      "\n",
      "Agent plays : (1, 1) \n",
      "\n",
      ". . . \n",
      ". O . \n",
      "X . . \n",
      "\n",
      "Agent plays : (2, 2) \n",
      "\n",
      ". . . \n",
      ". O . \n",
      "X . X \n",
      "\n",
      "Agent plays : (2, 1) \n",
      "\n",
      ". . . \n",
      ". O . \n",
      "X O X \n",
      "\n",
      "Agent plays : (0, 1) \n",
      "\n",
      ". X . \n",
      ". O . \n",
      "X O X \n",
      "\n",
      "Agent plays : (0, 0) \n",
      "\n",
      "O X . \n",
      ". O . \n",
      "X O X \n",
      "\n",
      "Agent plays : (1, 2) \n",
      "\n",
      "O X . \n",
      ". O X \n",
      "X O X \n",
      "\n",
      "Agent plays : (0, 2) \n",
      "\n",
      "O X O \n",
      ". O X \n",
      "X O X \n",
      "\n",
      "Agent plays : (1, 0) \n",
      "\n",
      "O X O \n",
      "X O X \n",
      "X O X \n",
      "\n",
      "Game over. Score : (0, 0)\n",
      "Even score.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.play_a_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0)\n",
      "3000 1381 701\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "grid_size = 3\n",
    "n_dim = 2\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, size=3, n_dim=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(size**n_dim,size**n_dim*2)\n",
    "        self.fc2 = torch.nn.Linear(size**n_dim*2, size**n_dim*2)\n",
    "        self.fc3 = torch.nn.Linear(size**n_dim*2, size**n_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.view(-1).type(torch.long)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.nn.ReLU()(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.nn.Softmax(dim=1)(x)\n",
    "        return x\n",
    "\n",
    "agent1 = Model() # randomly generate weight\n",
    "agent2 = Model()\n",
    "optimizer = torch.optim.SGD(agent1.parameters(), lr=0.01)\n",
    "\n",
    "game = Game(None, None)\n",
    "#print(\"transform \", game.board_position_to_index((2, 1, 1)))\n",
    "matches_result = []\n",
    "\n",
    "def get_best_possible(board, probs):\n",
    "    probs = [(probs[i], i) for i in range(probs.shape[0])]\n",
    "    probs = sorted(probs, reverse=True)\n",
    "    for j in probs:\n",
    "        position = j[1]\n",
    "        resulting_position = game.board_position_to_tuple(position)\n",
    "        if board[resulting_position] == 0:\n",
    "            return (position, resulting_position)\n",
    "    return None\n",
    "\n",
    "def make_move(game, model):\n",
    "    prev_state = np.reshape(game.board, (1, -1))\n",
    "    state = torch.tensor(game.board).view(1, -1).type(torch.float)\n",
    "    #print(state.shape)\n",
    "    result = model(state).detach().numpy()[0]\n",
    "    best_move, best_move_tuple = get_best_possible(game.board, result)\n",
    "    new_state, reward, is_done, _ = game.step(best_move_tuple)\n",
    "    return prev_state, reward, best_move, new_state\n",
    "    # return self.board, reward, self.is_done(), None\n",
    "\n",
    "game_records = []\n",
    "\n",
    "def loss(states, actions, rewards, model):\n",
    "    predicts = torch.log(model(states))\n",
    "    # rework\n",
    "    losses = rewards * predicts[np.arange(len(actions)), actions]\n",
    "    loss = -losses.mean()\n",
    "    return loss\n",
    "\n",
    "loss_values = []\n",
    "num_of_iteration = 5000\n",
    "wins = 0\n",
    "draws = 0\n",
    "loses = 0\n",
    "for i in range(num_of_iteration):\n",
    "    game.reset()\n",
    "    game_record = []\n",
    "    while not game.is_done():\n",
    "        result = make_move(game, agent1)\n",
    "        game_record.append(result)\n",
    "        if game.is_done():\n",
    "            break\n",
    "        #result = make_move(game, agent2)\n",
    "        prev_state = np.reshape(game.board, (1, -1))\n",
    "        move = random_policy(game.board, '')\n",
    "        new_state, reward, is_done, _ = game.step(move)\n",
    "        game_record.append((prev_state, reward, game.board_position_to_index((move))))\n",
    "    #print(game.current_score)\n",
    "    wins += game.current_score[0]\n",
    "    loses += game.current_score[1]\n",
    "    if(game.current_score == (0,0)):\n",
    "        draws += 1\n",
    "    # after played game recalculate rewards\n",
    "    # to check why states being saved as list of list\n",
    "    for j in game_record:\n",
    "        game_records.append(j)\n",
    "    #game_records.append(game_record)\n",
    "\n",
    "    if i % 100 == 0 and i != 0:\n",
    "        optimizer.zero_grad()\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        for j in game_records:\n",
    "            #print(j[1][0])\n",
    "            states.append(j[0][0])\n",
    "            actions.append(j[2])\n",
    "            rewards.append(j[1])\n",
    "        states = torch.tensor(states).type(torch.float)\n",
    "        actions = torch.tensor(actions)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        l = loss(states, actions, rewards, agent1)\n",
    "        loss_values.append(l.detach().numpy())\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        game_records = []\n",
    "        #print(actions)\n",
    "        #print(rewards)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: Add net training on played games\n",
    "print(game.current_score)\n",
    "\n",
    "print(wins, loses, draws)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3224576\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn\n",
    "print(loss_values[0])\n",
    "#loss_values = [i[0] for i in loss_values]\n",
    "loss = []\n",
    "for i in loss_values:\n",
    "    loss.append(float(i))\n",
    "print(np.arange(len(loss_values)))\n",
    "seaborn.lineplot(np.arange(len(loss_values)), loss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RL_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}