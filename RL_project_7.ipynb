{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "meNNqqqlRlN4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from sympy.utilities.iterables import subsets\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "from scipy.special import comb\n",
    "import pandas as pd\n",
    "import gym\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_almost_align(board, s):\n",
    "    size = board.shape[0]\n",
    "    for irow in range(size):\n",
    "        if board[irow, :].sum() == s * (size - 1):\n",
    "            i_empty = np.where(board[irow, :] == 0)[0].item()\n",
    "            return (irow, i_empty)\n",
    "    for icol in range(size):\n",
    "        if board[:, icol].sum() == s * (size - 1):\n",
    "            i_empty = np.where(board[:, icol] == 0)[0].item()\n",
    "            return (i_empty, icol)\n",
    "    if np.diag(board).sum() == s * (size - 1):\n",
    "        i_empty = np.where(np.diag(board) == 0)[0].item()\n",
    "        return (i_empty, i_empty)\n",
    "    if np.diag(np.rot90(board)).sum() == s * (size - 1):\n",
    "        for i in range(size):\n",
    "            if board[i, size - 1 - i] == 0:\n",
    "                return (i, size - 1 - i)\n",
    "    return None\n",
    "\n",
    "def random_policy(board, symbol):\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    return tuple(available_pos[np.random.randint(available_pos.shape[0])])\n",
    "\n",
    "def linear_policy(board, symbol):\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    return tuple(available_pos[0])\n",
    "\n",
    "def advanced_static_policy(board, symbol):\n",
    "    coords = check_almost_align(board, symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    coords = check_almost_align(board, -symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    return tuple(available_pos[0])\n",
    "\n",
    "def advanced_random_policy(board, symbol):\n",
    "    coords = check_almost_align(board, symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    coords = check_almost_align(board, -symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    return tuple(available_pos[np.random.randint(available_pos.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_Wavup5eRlOB"
   },
   "outputs": [],
   "source": [
    "def sarsa(game, agent, opponent_policy, alpha, alpha_factor, gamma, epsilon, epsilon_factor, \\\n",
    "          r_win, r_lose, r_even, r_even2, num_episodes):\n",
    "    for episode_index in range(num_episodes):\n",
    "        if episode_index % 1000 == 0:\n",
    "            print('.', end='')\n",
    "        alpha *= alpha_factor\n",
    "        epsilon *= epsilon_factor\n",
    "        state = game.reset()\n",
    "        if episode_index%2 == 1:\n",
    "            action = opponent_policy(state, game.turn)\n",
    "            state, _, _, _ = game.step(action)\n",
    "        action = agent.epsilon_greedy_policy(state, epsilon)\n",
    "        state_history = [state.copy()]\n",
    "        action_history = [action]\n",
    "        while True:\n",
    "            ################### The agent ignores this happens UNLESS it ends the game\n",
    "            intermediate_state, reward, done, _ = game.step(action)\n",
    "            if done:\n",
    "                if reward > 0:\n",
    "                    final_reward = r_win\n",
    "                else:\n",
    "                    final_reward = r_even\n",
    "                break\n",
    "            intermediate_action = opponent_policy(intermediate_state, game.turn)\n",
    "            ################### ------------------------------------------------\n",
    "            state, reward, done, _ = game.step(intermediate_action)\n",
    "            if done:\n",
    "                if reward > 0:\n",
    "                    reward = r_lose\n",
    "                else:\n",
    "                    reward = r_even2\n",
    "                break\n",
    "            action = agent.epsilon_greedy_policy(state, epsilon)\n",
    "            state_history.append(state.copy())\n",
    "            action_history.append(action)\n",
    "            \n",
    "        agent.update_Qtable(state_history, action_history, reward, alpha, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yi4p1W_wRlOC"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, size=3, policy=None):\n",
    "        self.q_array = pd.DataFrame(columns=[str(i)+str(j) for i in range(size) for j in range(size)])\n",
    "        self.policy = policy\n",
    "    \n",
    "    def update_Qtable(self, state_history, action_history, reward, alpha, gamma): \n",
    "        code_last_state, code_last_action = self.encode_state_and_action(state_history[-1], action_history[-1])\n",
    "        try:\n",
    "            self.q_array.loc[code_last_state]\n",
    "        except:\n",
    "            self.q_array.loc[code_last_state] = 0.5\n",
    "        self.q_array.loc[code_last_state, code_last_action] = reward\n",
    "        \n",
    "        for i in range(len(state_history)-2, -1, -1):\n",
    "            state = state_history[i]\n",
    "            new_state = state_history[i+1]\n",
    "            action = action_history[i]\n",
    "            new_action = action_history[i+1]\n",
    "            code_state, code_action = self.encode_state_and_action(state, action)\n",
    "            try:\n",
    "                self.q_array.loc[code_state]\n",
    "            except:\n",
    "                self.q_array.loc[code_state] = 0.5\n",
    "            code_new_state, code_new_action = self.encode_state_and_action(new_state, new_action)\n",
    "            self.q_array.loc[code_state, code_action] = (1 - alpha) * self.q_array.loc[code_state, code_action] \\\n",
    "                                                        + alpha * gamma * self.q_array.loc[code_new_state, code_new_action]\n",
    "    \n",
    "    def play_vs_opponent(self, state, symbol):\n",
    "        if self.policy is not None:\n",
    "            return self.policy(state, symbol)\n",
    "        state_code = self.encode_state(state)\n",
    "        try:\n",
    "            self.q_array.loc[state_code]\n",
    "        except:\n",
    "            legal_moves = np.argwhere(state == 0)\n",
    "            return tuple(legal_moves[np.random.randint(legal_moves.shape[0])])\n",
    "        ###### Find best move\n",
    "        potential_actions = self.q_array.loc[state_code]\n",
    "        reference_state = self.decode_one_state(state_code)\n",
    "        free_spots_scores = []\n",
    "        \n",
    "        if np.prod(state == reference_state):\n",
    "            for a in potential_actions.index:\n",
    "                empty_board = state * 0\n",
    "                empty_board[self.decode_action(a)] = 1\n",
    "                tr_empty_board = empty_board\n",
    "                tr_coords = tuple(np.argwhere(tr_empty_board == 1)[0])\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == np.rot90(reference_state, 1)):\n",
    "            for a in potential_actions.index:\n",
    "                empty_board = state * 0\n",
    "                empty_board[self.decode_action(a)] = 1\n",
    "                tr_empty_board = np.rot90(empty_board, 1)\n",
    "                tr_coords = tuple(np.argwhere(tr_empty_board == 1)[0])\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == np.rot90(reference_state, 2)):\n",
    "            for a in potential_actions.index:\n",
    "                empty_board = state * 0\n",
    "                empty_board[self.decode_action(a)] = 1\n",
    "                tr_empty_board = np.rot90(empty_board, 2)\n",
    "                tr_coords = tuple(np.argwhere(tr_empty_board == 1)[0])\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == np.rot90(reference_state, 3)):\n",
    "            for a in potential_actions.index:\n",
    "                empty_board = state * 0\n",
    "                empty_board[self.decode_action(a)] = 1\n",
    "                tr_empty_board = np.rot90(empty_board, 3)\n",
    "                tr_coords = tuple(np.argwhere(tr_empty_board == 1)[0])\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == reference_state[::-1]):\n",
    "            for a in potential_actions.index:\n",
    "                empty_board = state * 0\n",
    "                empty_board[self.decode_action(a)] = 1\n",
    "                tr_empty_board = empty_board[::-1]\n",
    "                tr_coords = tuple(np.argwhere(tr_empty_board == 1)[0])\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == reference_state[:, ::-1]):\n",
    "            for a in potential_actions.index:\n",
    "                empty_board = state * 0\n",
    "                empty_board[self.decode_action(a)] = 1\n",
    "                tr_empty_board = empty_board[:, ::-1]\n",
    "                tr_coords = tuple(np.argwhere(tr_empty_board == 1)[0])\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == reference_state.T):\n",
    "            for a in potential_actions.index:\n",
    "                empty_board = state * 0\n",
    "                empty_board[self.decode_action(a)] = 1\n",
    "                tr_empty_board = empty_board.T\n",
    "                tr_coords = tuple(np.argwhere(tr_empty_board == 1)[0])\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        \n",
    "        elif np.prod(state == np.rot90(reference_state, 2).T):\n",
    "            for a in potential_actions.index:\n",
    "                empty_board = state * 0\n",
    "                empty_board[self.decode_action(a)] = 1\n",
    "                tr_empty_board = np.rot90(empty_board, 2).T\n",
    "                tr_coords = tuple(np.argwhere(tr_empty_board == 1)[0])\n",
    "                if state[tr_coords] == 0:\n",
    "                    free_spots_scores.append((tr_coords, potential_actions.loc[a]))\n",
    "        else:\n",
    "            print('Error in play_vs_opponent in Agent !!!')\n",
    "        max_reward = max(free_spots_scores, key=lambda x: x[1])[1]\n",
    "        best_actions = [act for act, rew in free_spots_scores if rew == max_reward]\n",
    "        return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "    def greedy_policy(self, state):\n",
    "        \"\"\"\n",
    "        Return the next action as a tuple\n",
    "        \"\"\"\n",
    "        code_state = self.encode_state(state)\n",
    "        try:\n",
    "            self.q_array.loc[code_state]\n",
    "        except:\n",
    "            self.q_array.loc[code_state] = 0.5\n",
    "        reference_state = self.decode_one_state(code_state)\n",
    "        legal_actions = []\n",
    "        actions = self.q_array.loc[code_state]\n",
    "            \n",
    "        if np.prod(state == reference_state):\n",
    "            for act in actions.index:\n",
    "                coords_act = self.decode_action(act)\n",
    "                empty_board = state * 0\n",
    "                empty_board[coords_act] = 1\n",
    "                empty_board_transform = empty_board\n",
    "                coords_transform = tuple(np.argwhere(empty_board_transform == 1)[0])\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == np.rot90(reference_state, 1)):\n",
    "            for act in actions.index:\n",
    "                coords_act = self.decode_action(act)\n",
    "                empty_board = state * 0\n",
    "                empty_board[coords_act] = 1\n",
    "                empty_board_transform = np.rot90(empty_board, 1)\n",
    "                coords_transform = tuple(np.argwhere(empty_board_transform == 1)[0])\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == np.rot90(reference_state, 2)):\n",
    "            for act in actions.index:\n",
    "                coords_act = self.decode_action(act)\n",
    "                empty_board = state * 0\n",
    "                empty_board[coords_act] = 1\n",
    "                empty_board_transform = np.rot90(empty_board, 2)\n",
    "                coords_transform = tuple(np.argwhere(empty_board_transform == 1)[0])\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == np.rot90(reference_state, 3)):\n",
    "            for act in actions.index:\n",
    "                coords_act = self.decode_action(act)\n",
    "                empty_board = state * 0\n",
    "                empty_board[coords_act] = 1\n",
    "                empty_board_transform = np.rot90(empty_board, 3)\n",
    "                coords_transform = tuple(np.argwhere(empty_board_transform == 1)[0])\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == reference_state[::-1]):\n",
    "            for act in actions.index:\n",
    "                coords_act = self.decode_action(act)\n",
    "                empty_board = state * 0\n",
    "                empty_board[coords_act] = 1\n",
    "                empty_board_transform = empty_board[::-1]\n",
    "                coords_transform = tuple(np.argwhere(empty_board_transform == 1)[0])\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == reference_state[:, ::-1]):\n",
    "            for act in actions.index:\n",
    "                coords_act = self.decode_action(act)\n",
    "                empty_board = state * 0\n",
    "                empty_board[coords_act] = 1\n",
    "                empty_board_transform = empty_board[:, ::-1]\n",
    "                coords_transform = tuple(np.argwhere(empty_board_transform == 1)[0])\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == reference_state.T):\n",
    "            for act in actions.index:\n",
    "                coords_act = self.decode_action(act)\n",
    "                empty_board = state * 0\n",
    "                empty_board[coords_act] = 1\n",
    "                empty_board_transform = empty_board.T\n",
    "                coords_transform = tuple(np.argwhere(empty_board_transform == 1)[0])\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        \n",
    "        if np.prod(state == np.rot90(reference_state, 2).T):\n",
    "            for act in actions.index:\n",
    "                coords_act = self.decode_action(act)\n",
    "                empty_board = state * 0\n",
    "                empty_board[coords_act] = 1\n",
    "                empty_board_transform = np.rot90(empty_board, 2).T\n",
    "                coords_transform = tuple(np.argwhere(empty_board_transform == 1)[0])\n",
    "                if state[coords_transform] == 0:\n",
    "                    legal_actions.append((coords_transform, actions.loc[act]))\n",
    "            max_reward = max(legal_actions, key=lambda x: x[1])[1]\n",
    "            best_actions = [act_rew[0] for act_rew in legal_actions if act_rew[1] == max_reward]\n",
    "            return best_actions[np.random.randint(len(best_actions))]\n",
    "        print('\\nError in greedy policy !!!\\n')\n",
    "        print(state, '\\n')\n",
    "        print(reference_state, '\\n')\n",
    "        print(reference_state[::-1][::-1].T)\n",
    "\n",
    "    def epsilon_greedy_policy(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Return the next action as a tuple\n",
    "        \"\"\"\n",
    "        if self.q_array.empty or np.random.rand() < epsilon:\n",
    "            legal_moves = np.argwhere(state == 0)\n",
    "            size = legal_moves.shape[0]\n",
    "            random_idx = np.random.randint(size)\n",
    "            action = tuple(legal_moves[random_idx])\n",
    "        else:\n",
    "            action = self.greedy_policy(state)\n",
    "        return action\n",
    "    \n",
    "    def encode_action(self, action):\n",
    "        if type(action) is str:\n",
    "            return action\n",
    "        code_action = ''\n",
    "        for i in action:\n",
    "            code_action += str(i)\n",
    "        return code_action\n",
    "\n",
    "    def decode_action(self, code_action):\n",
    "        return tuple([int(i) for i in code_action])\n",
    "\n",
    "    def encode_one_state(self, state):\n",
    "        code_state = ''\n",
    "        for i in state.flatten():\n",
    "            code_state += str(i) if i != -1 else '2'\n",
    "        return code_state\n",
    "\n",
    "    def generate_all_sym_states(self, state):\n",
    "        sym_states = [self.encode_one_state(state), self.encode_one_state(np.rot90(state, 1)), \\\n",
    "                      self.encode_one_state(np.rot90(state, 2)), self.encode_one_state(np.rot90(state, 3)), \\\n",
    "                      self.encode_one_state(state[::-1]), self.encode_one_state(state[:, ::-1]), \\\n",
    "                      self.encode_one_state(state.T), self.encode_one_state(np.rot90(state, 2).T)]\n",
    "        sym_states.sort()\n",
    "        return sym_states\n",
    "    \n",
    "    def encode_state_and_action(self, state, action):\n",
    "        empty_state = state * 0\n",
    "        empty_state[action] = 1\n",
    "        sym = [(self.encode_one_state(state), tuple(np.argwhere(empty_state == 1)[0])), \\\n",
    "               (self.encode_one_state(np.rot90(state, 1)), tuple(np.argwhere(np.rot90(empty_state, 1) == 1)[0])), \\\n",
    "               (self.encode_one_state(np.rot90(state, 2)), tuple(np.argwhere(np.rot90(empty_state, 2) == 1)[0])), \\\n",
    "               (self.encode_one_state(np.rot90(state, 3)), tuple(np.argwhere(np.rot90(empty_state, 3) == 1)[0])), \\\n",
    "               (self.encode_one_state(state[::-1]), tuple(np.argwhere(empty_state[::-1] == 1)[0])), \\\n",
    "               (self.encode_one_state(state[:, ::-1]), tuple(np.argwhere(empty_state[:, ::-1] == 1)[0])), \\\n",
    "               (self.encode_one_state(state.T), tuple(np.argwhere(empty_state.T == 1)[0])), \\\n",
    "               (self.encode_one_state(np.rot90(state, 2).T), tuple(np.argwhere(np.rot90(empty_state, 2).T == 1)[0]))]\n",
    "        sym.sort(key=lambda x: x[0])\n",
    "        code_state = sym[0][0]\n",
    "        tr_action = sym[0][1]\n",
    "        code_action = self.encode_action(tr_action)\n",
    "        return code_state, code_action\n",
    "\n",
    "    def encode_state(self, state):\n",
    "        sym_states = self.generate_all_sym_states(state)\n",
    "        return sym_states[0]\n",
    "    \n",
    "    def decode_one_state(self, code_state):\n",
    "        flat_list = [0 if elem == '0' else 1 if elem == '1' else -1 for elem in list(code_state)]\n",
    "        size = int(np.sqrt(len(code_state)))\n",
    "        state = np.reshape(flat_list, (size, size))\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "LhVXcQs8RlOD"
   },
   "outputs": [],
   "source": [
    "class Game(gym.Env):\n",
    "    \n",
    "    def __init__(self, p1, p2, size=3, n_dim=2):\n",
    "        assert(type(n_dim) is int and n_dim >= 2), \"wrong n_dim\"\n",
    "        assert(type(size) is int and size >= 2), \"wrong size\"\n",
    "        self.n_dim = n_dim\n",
    "        self.size = size\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.turn = 1\n",
    "        self.board = np.zeros([size]*n_dim, dtype=int)\n",
    "        self.current_score = (0, 0)\n",
    "        self._max_episode_steps = 1000\n",
    "        super(Game, self).__init__()\n",
    "    \n",
    "    def simulate_games(self, n=100):\n",
    "        win_p1, win_p2, tot_even = 0, 0, 0\n",
    "        for _ in range(n // 2):\n",
    "            s1, s2 = self.play_a_game(verbose=False)\n",
    "            if s1 > s2:\n",
    "                win_p1 += 1\n",
    "            elif s2 > s1:\n",
    "                win_p2 += 1\n",
    "            else:\n",
    "                tot_even += 1\n",
    "        self.p1, self.p2 = self.p2, self.p1\n",
    "        for _ in range(n // 2):\n",
    "            s1, s2 = self.play_a_game(verbose=False)\n",
    "            if s1 > s2:\n",
    "                win_p2 += 1\n",
    "            elif s2 > s1:\n",
    "                win_p1 += 1\n",
    "            else:\n",
    "                tot_even += 1\n",
    "        self.p1, self.p2 = self.p2, self.p1\n",
    "        return win_p1, win_p2, tot_even\n",
    "    \n",
    "    def play_a_game(self, verbose=True):\n",
    "        self.reset()\n",
    "        if verbose:\n",
    "            self.render()\n",
    "        digits = '1234567890'\n",
    "        while not self.is_done():\n",
    "            if self.turn == 1:\n",
    "                player = self.p1\n",
    "            else:\n",
    "                player = self.p2\n",
    "            if isinstance(player, Agent):\n",
    "                coords = player.play_vs_opponent(self.board, self.turn)\n",
    "                if verbose:\n",
    "                    print('Agent plays :', coords, '\\n')\n",
    "            else:\n",
    "                coords_str = input('Coordinates of next move : ')\n",
    "                print()\n",
    "                coords = []\n",
    "                for c in coords_str:\n",
    "                    if c in digits:\n",
    "                        coords.append(int(c))\n",
    "                coords = tuple(coords)\n",
    "                while len(coords) != self.n_dim or max(coords) >= self.size or not self.is_available(coords):\n",
    "                    coords_str = input('Position not available, please try another one : ')\n",
    "                    coords = []\n",
    "                    for c in coords_str:\n",
    "                        if c in digits:\n",
    "                            coords.append(int(c))\n",
    "                    coords = tuple(coords)\n",
    "            self.step(coords)\n",
    "            if verbose:\n",
    "                self.render()\n",
    "        if verbose:\n",
    "            print('Game over. Score :', self.current_score)\n",
    "            if self.current_score[0] > self.current_score[1]:\n",
    "                print(self.p1, 'wins !')\n",
    "            elif self.current_score[1] > self.current_score[0]:\n",
    "                print(self.p2, 'wins !')\n",
    "            else:\n",
    "                print('Even score.')\n",
    "        return self.current_score\n",
    "    \n",
    "    def is_available(self, position):\n",
    "        return self.board[position] == 0\n",
    "    \n",
    "    def is_done(self):\n",
    "        if self.n_dim == 2:\n",
    "            return sum(self.current_score) != 0 or 0 not in self.board\n",
    "        return 0 not in self.board\n",
    "    \n",
    "    def reset(self):\n",
    "        self.turn = 1\n",
    "        self.current_score = (0, 0)\n",
    "        self.board *= 0\n",
    "        return self.board.copy()\n",
    "\n",
    "    def step(self, position):\n",
    "        self.board[position] = self.turn\n",
    "        score_p1, score_p2 = self.score()\n",
    "        score_p1_diff, score_p2_diff =  score_p1 - self.current_score[0], score_p2 - self.current_score[1]\n",
    "        # update only the score of the player that did the latest move\n",
    "        if self.turn == 1:\n",
    "            self.current_score = (score_p1, self.current_score[1])\n",
    "        else:\n",
    "            self.current_score = (self.current_score[0], score_p2)\n",
    "        reward = score_p1_diff if self.turn == 1 else score_p2_diff\n",
    "        self.turn *= -1\n",
    "        return self.board, reward, self.is_done(), None             \n",
    "    \n",
    "    def render(self):\n",
    "        visual_board = self.board.copy()\n",
    "        visual_board = np.where(visual_board == -1, 'O', visual_board)\n",
    "        visual_board = np.where(visual_board == '1', 'X', visual_board)\n",
    "        visual_board = np.where(visual_board == '0', '.', visual_board)\n",
    "        for icol in range(self.size):\n",
    "            for row in visual_board[icol, :]:\n",
    "                print(row, end=' ')\n",
    "            print()\n",
    "        print()\n",
    "    \n",
    "    def score(self):\n",
    "        score_p1 = 0\n",
    "        score_p2 = 0\n",
    "        \n",
    "        def slice_to_mask(L):\n",
    "            \"\"\"\n",
    "            Enables to use slicing operator like array[x, y, :, z] with choosing the position\n",
    "            of the symbol ':' (represented with a -1 instead). For example L can be equal to\n",
    "            [0, 0, -1, 0] if we want to access self.board[0, 0, :, 0]\n",
    "            \"\"\"\n",
    "            mask = np.zeros([self.size] * self.n_dim, dtype=bool)\n",
    "            dim = L.index(-1)\n",
    "            for tile in range(self.size):\n",
    "                L[dim] = tile\n",
    "                mask[tuple(L)] = True\n",
    "            return mask\n",
    "        \n",
    "        # vertical and horizontal axis\n",
    "        all_axis = []\n",
    "        for d in range(self.size ** self.n_dim):\n",
    "            all_axis.append([(d // self.size**k) % self.size for k in range(self.n_dim)[::-1]])\n",
    "            # example in 3D case with size 3 :\n",
    "            # all_axis = [ [i, j, k] for i = 0, 1, 2 for j = 0, 1, 2 for k = 0, 1, 2 ]\n",
    "        for d in range(self.n_dim):\n",
    "            d_axis = np.array(all_axis)\n",
    "            d_axis[:, d] = -1\n",
    "            d_axis = np.unique(d_axis, axis=0)\n",
    "            for axis in d_axis:\n",
    "                space_mask = slice_to_mask(list(axis))\n",
    "                in_game_axis = self.board[space_mask]\n",
    "                axis_value = in_game_axis.sum().item()\n",
    "                if axis_value == self.size:\n",
    "                    score_p1 += 1\n",
    "                elif axis_value == -self.size:\n",
    "                    score_p2 += 1\n",
    "        \n",
    "        # diagonal axis\n",
    "        diag = np.array([range(self.size)]).T\n",
    "        antidiag = np.array([range(self.size-1, -1, -1)]).T\n",
    "        poss_diag = np.array([diag, antidiag])\n",
    "        poss_index = list(range(self.size))\n",
    "        coords_to_check = set()\n",
    "        for dof in range(self.n_dim-2, -1, -1):\n",
    "            dof_fc = self.n_dim - dof\n",
    "            cpt = 0\n",
    "            for fc in subsets(poss_diag, dof_fc, repetition=True):\n",
    "                if cpt == int(dof_fc / 2) + 1:\n",
    "                    break\n",
    "                cpt += 1\n",
    "                frozen_comp = np.array(fc).reshape((dof_fc, self.size)).T\n",
    "                if dof > 0:\n",
    "                    for free_comp in subsets(poss_index, dof, repetition=True):\n",
    "                        free_comp_array = np.repeat(np.array([free_comp]), self.size, axis=0)\n",
    "                        coords = np.hstack((free_comp_array, frozen_comp))\n",
    "                        for perm in multiset_permutations(coords.T.tolist()):\n",
    "                            perm_coords = [list(i) for i in zip(*perm)]\n",
    "                            perm_coords.sort()\n",
    "                            coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                else:\n",
    "                    coords = frozen_comp\n",
    "                    for perm in multiset_permutations(coords.T.tolist()):\n",
    "                        perm_coords = [list(i) for i in zip(*perm)]\n",
    "                        perm_coords.sort()\n",
    "                        coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                        \n",
    "        for coords in coords_to_check:\n",
    "            total = 0\n",
    "            for tile in coords:\n",
    "                total += self.board[tile]\n",
    "            if abs(total) == self.size:\n",
    "                if total > 0:\n",
    "                    score_p1 += 1\n",
    "                else:\n",
    "                    score_p2 += 1\n",
    "                \n",
    "        return score_p1, score_p2\n",
    "    \n",
    "    def almost_align(self):\n",
    "        \n",
    "        def slice_to_mask(L):\n",
    "            \"\"\"\n",
    "            Enables to use slicing operator like array[x, y, :, z] with choosing the position\n",
    "            of the symbol ':' (represented with a -1 instead). For example L can be equal to\n",
    "            [0, 0, -1, 0] if we want to access self.board[0, 0, :, 0]\n",
    "            \"\"\"\n",
    "            mask = np.zeros([self.size] * self.n_dim, dtype=bool)\n",
    "            dim = L.index(-1)\n",
    "            for tile in range(self.size):\n",
    "                L[dim] = tile\n",
    "                mask[tuple(L)] = True\n",
    "            return mask\n",
    "        \n",
    "        score_p1 = 0\n",
    "        score_p2 = 0\n",
    "        # vertical and horizontal axis\n",
    "        all_axis = []\n",
    "        for d in range(self.size ** self.n_dim):\n",
    "            all_axis.append([(d // self.size**k) % self.size for k in range(self.n_dim)[::-1]])\n",
    "            # example in 3D case with size 3 :\n",
    "            # all_axis = [ [i, j, k] for i = 0, 1, 2 for j = 0, 1, 2 for k = 0, 1, 2 ]\n",
    "        for d in range(self.n_dim):\n",
    "            d_axis = np.array(all_axis)\n",
    "            d_axis[:, d] = -1\n",
    "            d_axis = np.unique(d_axis, axis=0)\n",
    "            for axis in d_axis:\n",
    "                space_mask = slice_to_mask(list(axis))\n",
    "                in_game_axis = self.board[space_mask]\n",
    "                axis_value = in_game_axis.sum().item()\n",
    "                if axis_value == self.size - 1:\n",
    "                    score_p1 += 1\n",
    "                elif axis_value == -self.size + 1:\n",
    "                    score_p2 += 1\n",
    "        \n",
    "        # diagonal axis\n",
    "        diag = np.array([range(self.size)]).T\n",
    "        antidiag = np.array([range(self.size-1, -1, -1)]).T\n",
    "        poss_diag = np.array([diag, antidiag])\n",
    "        poss_index = list(range(self.size))\n",
    "        coords_to_check = set()\n",
    "        for dof in range(self.n_dim-2, -1, -1):\n",
    "            dof_fc = self.n_dim - dof\n",
    "            cpt = 0\n",
    "            for fc in subsets(poss_diag, dof_fc, repetition=True):\n",
    "                if cpt == int(dof_fc / 2) + 1:\n",
    "                    break\n",
    "                cpt += 1\n",
    "                frozen_comp = np.array(fc).reshape((dof_fc, self.size)).T\n",
    "                if dof > 0:\n",
    "                    for free_comp in subsets(poss_index, dof, repetition=True):\n",
    "                        free_comp_array = np.repeat(np.array([free_comp]), self.size, axis=0)\n",
    "                        coords = np.hstack((free_comp_array, frozen_comp))\n",
    "                        for perm in multiset_permutations(coords.T.tolist()):\n",
    "                            perm_coords = [list(i) for i in zip(*perm)]\n",
    "                            perm_coords.sort()\n",
    "                            coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                else:\n",
    "                    coords = frozen_comp\n",
    "                    for perm in multiset_permutations(coords.T.tolist()):\n",
    "                        perm_coords = [list(i) for i in zip(*perm)]\n",
    "                        perm_coords.sort()\n",
    "                        coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                        \n",
    "        for coords in coords_to_check:\n",
    "            total = 0\n",
    "            for tile in coords:\n",
    "                total += self.board[tile]\n",
    "            if abs(total) == self.size - 1:\n",
    "                if total > 0:\n",
    "                    score_p1 += 1\n",
    "                else:\n",
    "                    score_p2 += 1\n",
    "                \n",
    "        return score_p1, score_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................Wall time: 8min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "agent = Agent(size=3)\n",
    "random_agent = Agent(size=3, policy=advanced_random_policy)\n",
    "game = Game(agent, random_agent, n_dim=2, size=3)\n",
    "sarsa(game, agent, random_policy, alpha=0.8, alpha_factor=0.9999, gamma=0.9, epsilon=1.0, epsilon_factor=0.9998, \\\n",
    "      r_win=5.0, r_lose=0.0, r_even=1.0, r_even2=1.5, num_episodes=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent won 237 times, lost 0 times and did 763 even games\n"
     ]
    }
   ],
   "source": [
    "win_p1, winp2, tot_even = game.simulate_games(1000)\n",
    "print('Agent won', win_p1, 'times, lost', winp2, 'times and did', tot_even, 'even games')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.p1, game.p2 = game.p2, game.p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . \n",
      ". . . \n",
      ". . . \n",
      "\n",
      "Agent plays : (2, 0) \n",
      "\n",
      ". . . \n",
      ". . . \n",
      "X . . \n",
      "\n",
      "Agent plays : (1, 1) \n",
      "\n",
      ". . . \n",
      ". O . \n",
      "X . . \n",
      "\n",
      "Agent plays : (2, 2) \n",
      "\n",
      ". . . \n",
      ". O . \n",
      "X . X \n",
      "\n",
      "Agent plays : (2, 1) \n",
      "\n",
      ". . . \n",
      ". O . \n",
      "X O X \n",
      "\n",
      "Agent plays : (0, 1) \n",
      "\n",
      ". X . \n",
      ". O . \n",
      "X O X \n",
      "\n",
      "Agent plays : (0, 0) \n",
      "\n",
      "O X . \n",
      ". O . \n",
      "X O X \n",
      "\n",
      "Agent plays : (1, 2) \n",
      "\n",
      "O X . \n",
      ". O X \n",
      "X O X \n",
      "\n",
      "Agent plays : (0, 2) \n",
      "\n",
      "O X O \n",
      ". O X \n",
      "X O X \n",
      "\n",
      "Agent plays : (1, 0) \n",
      "\n",
      "O X O \n",
      "X O X \n",
      "X O X \n",
      "\n",
      "Game over. Score : (0, 0)\n",
      "Even score.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.play_a_game()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RL_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
