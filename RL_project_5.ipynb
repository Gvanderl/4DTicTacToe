{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "meNNqqqlRlN4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from sympy.utilities.iterables import subsets\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "from scipy.special import comb\n",
    "import pandas as pd\n",
    "import gym\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "\n",
    "# %load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(board, symbol):\n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    return tuple(available_pos[np.random.randint(available_pos.shape[0])])\n",
    "\n",
    "def deterministic_policy(board, symbol):\n",
    "    \n",
    "    def check_almost_align(board, s):\n",
    "        size = board.shape[0]\n",
    "        for irow in range(size):\n",
    "            if board[irow, :].sum() == s * (size - 1):\n",
    "                i_empty = np.where(board[irow, :] == 0)[0].item()\n",
    "                return (irow, i_empty)\n",
    "        for icol in range(size):\n",
    "            if board[:, icol].sum() == s * (size - 1):\n",
    "                i_empty = np.where(board[:, icol] == 0)[0].item()\n",
    "                return (i_empty, icol)\n",
    "        if np.diag(board).sum() == s * (size - 1):\n",
    "            i_empty = np.where(np.diag(board) == 0)[0].item()\n",
    "            return (i_empty, i_empty)\n",
    "        if np.diag(np.rot90(board)).sum() == s * (size - 1):\n",
    "            for i in range(size):\n",
    "                if board[i, size - 1 - i] == 0:\n",
    "                    return (i, size - 1 - i)\n",
    "        return None\n",
    "    \n",
    "    coords = check_almost_align(board, symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    coords = check_almost_align(board, -symbol)\n",
    "    if coords is not None:\n",
    "        return coords\n",
    "    \n",
    "    #size = board.shape[0]\n",
    "    #if board[size // 2, size // 2] == 0:\n",
    "    #    return (size // 2, size // 2)\n",
    "    \n",
    "    available_pos = np.array(np.where(game.board == 0)).T\n",
    "    return tuple(available_pos[np.random.randint(available_pos.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(game, agent, opponent_policy, alpha, alpha_factor, gamma, epsilon, epsilon_factor, \\\n",
    "               r_win, r_lose, r_even, r_even2, num_episodes):\n",
    "    \n",
    "    def algo(q_array, code_state, code_action, code_new_state, code_new_action, reward, alpha, gamma, state=None):\n",
    "        if code_new_state is None:\n",
    "            return q_array.loc[code_state, code_action] + alpha * (reward - q_array.loc[code_state, code_action])\n",
    "        potential_actions = q_array.loc[code_new_state, :]\n",
    "        free_spots_scores = [potential_actions.loc[a] for a in potential_actions.columns if state[Agent.decode(a)] == 0]\n",
    "        return q_array.loc[code_state, code_action] + alpha * (reward + gamma * np.max(free_spots_scores) \\\n",
    "                                                               - q_array.loc[code_state, code_action])  \n",
    "    agent.algo = algo\n",
    "    for episode_index in range(num_episodes):\n",
    "        if episode_index % 1000 == 0:\n",
    "            print('.', end='')\n",
    "        alpha *= alpha_factor\n",
    "        epsilon *= epsilon_factor\n",
    "        state = game.reset()\n",
    "        if episode_index%2 == 1:\n",
    "            action = opponent_policy(game.board, game.turn)\n",
    "            state, _, _, _ = game.step(action)\n",
    "        while True:\n",
    "            action = agent.epsilon_greedy_policy(state, epsilon)\n",
    "            ################### The agent ignores this happens UNLESS it ends the game\n",
    "            intermediate_state, final_reward, done, _ = game.step(action)\n",
    "            if done:\n",
    "                #### give a reward of 0.1 for an even game when the agent plays first\n",
    "                if final_reward > 0:\n",
    "                    final_reward = r_win\n",
    "                else:\n",
    "                    final_reward = r_even\n",
    "                agent.update_Qtable(state, action, None, None, final_reward, alpha, gamma)\n",
    "                break\n",
    "            intermediate_action = opponent_policy(intermediate_state, game.turn)\n",
    "            ################### ------------------------------------------------------\n",
    "            new_state, reward, done, _ = game.step(intermediate_action)\n",
    "            if done:\n",
    "                if reward > 0:\n",
    "                    reward = r_lose\n",
    "                else:\n",
    "                    reward = r_even2\n",
    "                agent.update_Qtable(state, action, new_state, None, reward, alpha, gamma)\n",
    "                break\n",
    "            agent.update_Qtable(state, action, new_state, None, reward, alpha, gamma)\n",
    "            state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "_Wavup5eRlOB"
   },
   "outputs": [],
   "source": [
    "def sarsa(game, agent, opponent_policy, alpha, alpha_factor, gamma, epsilon, epsilon_factor, \\\n",
    "          r_win, r_lose, r_even, r_even2, num_episodes):\n",
    "    \n",
    "    def algo(q_array, code_state, code_action, code_new_state, code_new_action, reward, alpha, gamma, state=None):\n",
    "        if code_new_state is None or code_new_action is None:\n",
    "            return q_array.loc[code_state, code_action] + alpha * (reward - q_array.loc[code_state, code_action])\n",
    "        return q_array.loc[code_state, code_action] + alpha * \\\n",
    "                (reward + gamma * q_array.loc[code_new_state, code_new_action] - q_array.loc[code_state, code_action])\n",
    "    agent.algo = algo\n",
    "    \n",
    "    tot_vict = 0\n",
    "    tot_even = 0\n",
    "    tot_lose = 0\n",
    "    \n",
    "    for episode_index in range(num_episodes):\n",
    "        if episode_index % 1000 == 0:\n",
    "            print('.', end='')\n",
    "        alpha *= alpha_factor\n",
    "        epsilon *= epsilon_factor\n",
    "        state = game.reset()\n",
    "        if episode_index%2 == 1:\n",
    "            action = opponent_policy(game.board, game.turn)\n",
    "            state, _, _, _ = game.step(action)\n",
    "        action = agent.epsilon_greedy_policy(state, epsilon)\n",
    "        while True:\n",
    "            ################### The agent ignores this happens UNLESS it ends the game\n",
    "            intermediate_state, final_reward, done, _ = game.step(action)\n",
    "            if done:\n",
    "                #### give a reward of 0.1 for an even game when the agent plays first\n",
    "                if final_reward > 0:\n",
    "                    tot_vict += 1\n",
    "                    final_reward = r_win\n",
    "                else:\n",
    "                    tot_even += 1\n",
    "                    final_reward = r_even\n",
    "                agent.update_Qtable(state, action, None, None, final_reward, alpha, gamma)\n",
    "                break\n",
    "            intermediate_action = opponent_policy(intermediate_state, game.turn)\n",
    "            ################### ------------------------------------------------------\n",
    "            new_state, reward, done, _ = game.step(intermediate_action)\n",
    "            if done:\n",
    "                if reward > 0:\n",
    "                    tot_lose += 1\n",
    "                    reward = r_lose\n",
    "                else:\n",
    "                    tot_even += 1\n",
    "                    reward = r_even2\n",
    "                agent.update_Qtable(state, action, None, None, reward, alpha, gamma)\n",
    "                break\n",
    "            new_action = agent.epsilon_greedy_policy(new_state, epsilon)\n",
    "            agent.update_Qtable(state, action, new_state, new_action, reward, alpha, gamma)\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "    \n",
    "    print('tot vict :', tot_vict, ' tot lose :', tot_lose, ' tot even :', tot_even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "yi4p1W_wRlOC"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, size=3, policy=None):\n",
    "        self.q_array = pd.DataFrame(columns=[str(i)+str(j) for i in range(size) for j in range(size)])\n",
    "        self.algo = None\n",
    "        self.policy = policy\n",
    "    \n",
    "    def update_Qtable(self, state, action, new_state, new_action, reward, alpha, gamma):        \n",
    "        code_state = self.encode_state(state)\n",
    "        code_action = self.encode_action(action)\n",
    "        #### create new rows for the new states met\n",
    "        try:\n",
    "            self.q_array.loc[code_state, code_action]\n",
    "        except:\n",
    "            self.q_array.loc[code_state, code_action] = 0\n",
    "            self.q_array = self.q_array.fillna(0)\n",
    "        ####\n",
    "        if new_state is not None and new_action is not None:\n",
    "            code_new_state = self.encode_state(new_state)\n",
    "            code_new_action = self.encode_action(new_action)\n",
    "            try:\n",
    "                self.q_array.loc[code_new_state, code_new_action]\n",
    "            except:\n",
    "                self.q_array.loc[code_new_state, code_new_action] = 0\n",
    "                self.q_array = self.q_array.fillna(0)\n",
    "        else:\n",
    "            code_new_state = None\n",
    "            code_new_action = None\n",
    "            \n",
    "        new_val = self.algo(self.q_array, code_state, code_action, code_new_state, code_new_action, reward, alpha, gamma, state)\n",
    "        self.q_array.loc[code_state, code_action] = new_val\n",
    "        \n",
    "    def play_vs_opponent(self, state, symbol):\n",
    "        if self.policy is not None:\n",
    "            return self.policy(state, symbol)\n",
    "        state_code = self.encode_state(state)\n",
    "        try:\n",
    "            possible_moves = self.q_array.loc[state_code, :]\n",
    "            best_move_score, best_move_idx = 0, None\n",
    "            for cod_act in possible_moves.index:\n",
    "                coords = self.decode_action(cod_act)\n",
    "                if state[coords] == 0 and possible_moves.loc[cod_act] >= best_move_score:\n",
    "                    best_move_score = possible_moves.loc[cod_act]\n",
    "                    best_move_idx = cod_act\n",
    "            return self.decode_action(best_move_idx)\n",
    "        except:\n",
    "            legal_moves = np.argwhere(state == 0)\n",
    "        if legal_moves.shape[0] <= 2:\n",
    "            return tuple(legal_moves[np.random.randint(legal_moves.shape[0])])\n",
    "        min_max_rewards = []\n",
    "        for action_array in legal_moves:\n",
    "            intermediate_state = state.copy()\n",
    "            intermediate_state[tuple(action_array)] = symbol\n",
    "            legal_moves_opponent = np.argwhere(intermediate_state == 0)\n",
    "            max_rewards = []\n",
    "            for opponent_action_array in legal_moves_opponent:\n",
    "                new_state = intermediate_state.copy()\n",
    "                new_state[tuple(opponent_action_array)] = -symbol\n",
    "                try:\n",
    "                    code_new_state = self.encode_state(new_state)\n",
    "                    max_reward = np.max(self.q_array.loc[code_state_code, :])\n",
    "                    max_rewards.append(max_reward)\n",
    "                except:\n",
    "                    pass\n",
    "            if max_rewards:\n",
    "                min_max_reward = np.min(max_rewards)\n",
    "                min_max_rewards.append(min_max_reward)\n",
    "        if min_max_rewards:\n",
    "            best_legal_move_idx = np.argmax(min_max_rewards)\n",
    "            return tuple(legal_moves[best_legal_move_idx])\n",
    "        return tuple(legal_moves[np.random.randint(legal_moves.shape[0])])\n",
    "        \n",
    "    def greedy_policy(self, state):\n",
    "        \"\"\"\n",
    "        Return the next action as a tuple\n",
    "        \"\"\"\n",
    "        code_state = self.encode_state(state)\n",
    "        try:\n",
    "            self.q_array.loc[code_sym_states]\n",
    "        except:\n",
    "            self.q_array.loc[code_sym_states] = 0\n",
    "            \n",
    "        actions = self.q_array.loc[code_sym_states]\n",
    "        legal_action_codes = [act for act in actions.index if state[self.decode_action(act)] == 0]\n",
    "        legal_actions = actions.loc[legal_action_codes]\n",
    "        best_legal_actions = legal_actions[legal_actions == legal_actions.max()]\n",
    "        code_action_taken = np.random.choice(best_legal_actions.index)\n",
    "        coords_action = self.decode_action(code_action_taken)\n",
    "        corresponding_state = self.decode_one_state(code_sym_states.split('_')[0])\n",
    "        \n",
    "        if np.prod(state == corresponding_state):\n",
    "            return coords_action\n",
    "        \n",
    "        empty_board = state * 0\n",
    "        empty_board[coords_action] = 1\n",
    "        \n",
    "        if np.prod(state == np.rot90(corresponding_state, 1)):\n",
    "            return tuple(np.argwhere(np.rot90(empty_board) == 1)[0])\n",
    "        \n",
    "        if np.prod(state == np.rot90(corresponding_state, 2)):\n",
    "            return tuple(np.argwhere(np.rot90(empty_board, 2) == 1)[0])\n",
    "        \n",
    "        if np.prod(state == np.rot90(corresponding_state, 3)):\n",
    "            return tuple(np.argwhere(np.rot90(empty_board, 3) == 1)[0])\n",
    "        \n",
    "        if np.prod(state == corresponding_state[::-1]):\n",
    "            return tuple(np.argwhere(empty_board[::-1] == 1)[0])\n",
    "        \n",
    "        if np.prod(state == corresponding_state[:, ::-1]):\n",
    "            return tuple(np.argwhere(empty_board[:, ::-1] == 1)[0])\n",
    "        \n",
    "        if np.prod(state == corresponding_state.T):\n",
    "            return tuple(np.argwhere(empty_board.T == 1)[0])\n",
    "        \n",
    "        if np.prod(state == corresponding_state[::-1, ::-1].T):\n",
    "            return tuple(np.argwhere(empty_board[::-1, ::-1].T == 1)[0])\n",
    "\n",
    "    def epsilon_greedy_policy(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Return the next action as a tuple\n",
    "        \"\"\"\n",
    "        if self.q_array.empty or np.random.rand() < epsilon:\n",
    "            legal_moves = np.argwhere(state == 0)\n",
    "            size = legal_moves.shape[0]\n",
    "            random_idx = np.random.randint(size)\n",
    "            action = tuple(legal_moves[random_idx])\n",
    "        else:\n",
    "            action = self.greedy_policy(state)\n",
    "        return action\n",
    "    \n",
    "    def encode_action(self, action):\n",
    "        if type(action) is str:\n",
    "            return action\n",
    "        code_action = ''\n",
    "        for i in action:\n",
    "            code_action += str(i)\n",
    "        return code_action\n",
    "\n",
    "    def decode_action(self, code_action):\n",
    "        return tuple([int(i) for i in code_action])\n",
    "\n",
    "    def encode_one_state(self, state):\n",
    "        code_state = ''\n",
    "        for i in state.flatten():\n",
    "            code_state += str(i) if i != -1 else '2'\n",
    "        return code_state\n",
    "\n",
    "    def generate_all_sym_states(self, state):\n",
    "        sym_states = [self.encode_one_state(state), self.encode_one_state(np.rot90(state, 1)), \\\n",
    "                      self.encode_one_state(np.rot90(state, 2)), self.encode_one_state(np.rot90(state, 3)), \\\n",
    "                      self.encode_one_state(state[::-1]), self.encode_one_state(state[:, ::-1]), \\\n",
    "                      self.encode_one_state(state.T), self.encode_one_state(state[::-1, ::-1].T)]\n",
    "        sym_states.sort()\n",
    "        return sym_states\n",
    "\n",
    "    def encode_state(self, state):\n",
    "        sym_states = self.generate_all_sym_states(state)\n",
    "        return sym_states[0]\n",
    "    \n",
    "    def decode_one_state(self, code_state):\n",
    "        flat_list = [0 if elem == '0' else 1 if elem == '1' else -1 for elem in list(code_state)]\n",
    "        state = np.reshape(flat_list, (self.size, self.size))\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "LhVXcQs8RlOD"
   },
   "outputs": [],
   "source": [
    "class Game(gym.Env):\n",
    "    \n",
    "    def __init__(self, p1, p2, size=3, n_dim=2):\n",
    "        assert(type(n_dim) is int and n_dim >= 2), \"wrong n_dim\"\n",
    "        assert(type(size) is int and size >= 2), \"wrong size\"\n",
    "        self.n_dim = n_dim\n",
    "        self.size = size\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.turn = 1\n",
    "        self.board = np.zeros([size]*n_dim, dtype=int)\n",
    "        self.current_score = (0, 0)\n",
    "        self._max_episode_steps = 1000\n",
    "        super(Game, self).__init__()\n",
    "    \n",
    "    def play_a_game(self):\n",
    "        self.turn = 1\n",
    "        self.reset()\n",
    "        self.render()\n",
    "        digits = '1234567890'\n",
    "        while not self.is_done():\n",
    "            if self.turn == 1:\n",
    "                player = self.p1\n",
    "            else:\n",
    "                player = self.p2\n",
    "            if isinstance(player, Agent):\n",
    "                coords = player.play_vs_opponent(self.board, self.turn)\n",
    "                print('Agent plays :', coords, '\\n')\n",
    "            else:\n",
    "                coords_str = input('Coordinates of next move : ')\n",
    "                print()\n",
    "                coords = []\n",
    "                for c in coords_str:\n",
    "                    if c in digits:\n",
    "                        coords.append(int(c))\n",
    "                coords = tuple(coords)\n",
    "                while len(coords) != self.n_dim or max(coords) >= self.size or not self.is_available(coords):\n",
    "                    coords_str = input('Position not available, please try another one : ')\n",
    "                    coords = []\n",
    "                    for c in coords_str:\n",
    "                        if c in digits:\n",
    "                            coords.append(int(c))\n",
    "                    coords = tuple(coords)\n",
    "            self.step(coords)\n",
    "            self.render()\n",
    "        print('Game over. Score :', self.current_score)\n",
    "        if self.current_score[0] > self.current_score[1]:\n",
    "            print(self.p1, 'wins !')\n",
    "        elif self.current_score[1] > self.current_score[0]:\n",
    "            print(self.p2, 'wins !')\n",
    "        else:\n",
    "            print('Even score.')\n",
    "    \n",
    "    def is_available(self, position):\n",
    "        return self.board[position] == 0\n",
    "    \n",
    "    def is_done(self):\n",
    "        if self.n_dim == 2:\n",
    "            return sum(self.current_score) != 0 or 0 not in self.board\n",
    "        return 0 not in self.board\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board *= 0\n",
    "        return self.board.copy()\n",
    "\n",
    "    def step(self, position):\n",
    "        self.board[position] = self.turn\n",
    "        score_p1, score_p2 = self.score()\n",
    "        score_p1_diff, score_p2_diff =  score_p1 - self.current_score[0], score_p2 - self.current_score[1]\n",
    "        # update only the score of the player that did the latest move\n",
    "        if self.turn == 1:\n",
    "            self.current_score = (score_p1, self.current_score[1])\n",
    "        else:\n",
    "            self.current_score = (self.current_score[0], score_p2)\n",
    "        reward = score_p1_diff if self.turn == 1 else score_p2_diff\n",
    "        self.turn *= -1\n",
    "        return self.board, reward, self.is_done(), None             \n",
    "    \n",
    "    def render(self):\n",
    "        visual_board = self.board.copy()\n",
    "        visual_board = np.where(visual_board == -1, 'O', visual_board)\n",
    "        visual_board = np.where(visual_board == '1', 'X', visual_board)\n",
    "        visual_board = np.where(visual_board == '0', '.', visual_board)\n",
    "        for icol in range(self.size):\n",
    "            for row in visual_board[icol, :]:\n",
    "                print(row, end=' ')\n",
    "            print()\n",
    "        print()\n",
    "    \n",
    "    def score(self):\n",
    "        score_p1 = 0\n",
    "        score_p2 = 0\n",
    "        \n",
    "        def slice_to_mask(L):\n",
    "            \"\"\"\n",
    "            Enables to use slicing operator like array[x, y, :, z] with choosing the position\n",
    "            of the symbol ':' (represented with a -1 instead). For example L can be equal to\n",
    "            [0, 0, -1, 0] if we want to access self.board[0, 0, :, 0]\n",
    "            \"\"\"\n",
    "            mask = np.zeros([self.size] * self.n_dim, dtype=bool)\n",
    "            dim = L.index(-1)\n",
    "            for tile in range(self.size):\n",
    "                L[dim] = tile\n",
    "                mask[tuple(L)] = True\n",
    "            return mask\n",
    "        \n",
    "        # vertical and horizontal axis\n",
    "        all_axis = []\n",
    "        for d in range(self.size ** self.n_dim):\n",
    "            all_axis.append([(d // self.size**k) % self.size for k in range(self.n_dim)[::-1]])\n",
    "            # example in 3D case with size 3 :\n",
    "            # all_axis = [ [i, j, k] for i = 0, 1, 2 for j = 0, 1, 2 for k = 0, 1, 2 ]\n",
    "        for d in range(self.n_dim):\n",
    "            d_axis = np.array(all_axis)\n",
    "            d_axis[:, d] = -1\n",
    "            d_axis = np.unique(d_axis, axis=0)\n",
    "            for axis in d_axis:\n",
    "                space_mask = slice_to_mask(list(axis))\n",
    "                in_game_axis = self.board[space_mask]\n",
    "                axis_value = in_game_axis.sum().item()\n",
    "                if axis_value == self.size:\n",
    "                    score_p1 += 1\n",
    "                elif axis_value == -self.size:\n",
    "                    score_p2 += 1\n",
    "        \n",
    "        # diagonal axis\n",
    "        diag = np.array([range(self.size)]).T\n",
    "        antidiag = np.array([range(self.size-1, -1, -1)]).T\n",
    "        poss_diag = np.array([diag, antidiag])\n",
    "        poss_index = list(range(self.size))\n",
    "        coords_to_check = set()\n",
    "        for dof in range(self.n_dim-2, -1, -1):\n",
    "            dof_fc = self.n_dim - dof\n",
    "            cpt = 0\n",
    "            for fc in subsets(poss_diag, dof_fc, repetition=True):\n",
    "                if cpt == int(dof_fc / 2) + 1:\n",
    "                    break\n",
    "                cpt += 1\n",
    "                frozen_comp = np.array(fc).reshape((dof_fc, self.size)).T\n",
    "                if dof > 0:\n",
    "                    for free_comp in subsets(poss_index, dof, repetition=True):\n",
    "                        free_comp_array = np.repeat(np.array([free_comp]), self.size, axis=0)\n",
    "                        coords = np.hstack((free_comp_array, frozen_comp))\n",
    "                        for perm in multiset_permutations(coords.T.tolist()):\n",
    "                            perm_coords = [list(i) for i in zip(*perm)]\n",
    "                            perm_coords.sort()\n",
    "                            coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                else:\n",
    "                    coords = frozen_comp\n",
    "                    for perm in multiset_permutations(coords.T.tolist()):\n",
    "                        perm_coords = [list(i) for i in zip(*perm)]\n",
    "                        perm_coords.sort()\n",
    "                        coords_to_check.add(tuple(map(tuple, perm_coords)))\n",
    "                        \n",
    "        for coords in coords_to_check:\n",
    "            total = 0\n",
    "            for tile in coords:\n",
    "                total += self.board[tile]\n",
    "            if abs(total) == self.size:\n",
    "                if total > 0:\n",
    "                    score_p1 += 1\n",
    "                else:\n",
    "                    score_p2 += 1\n",
    "                \n",
    "        return score_p1, score_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........tot vict : 2144  tot lose : 2829  tot even : 5027\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(size=3)\n",
    "game = Game('Vic', agent, n_dim=2, size=3)\n",
    "sarsa(game, agent, random_policy, alpha=0.5, alpha_factor=1.0, gamma=0.9, epsilon=1.0, epsilon_factor=0.9997, \\\n",
    "      r_win=100, r_lose=0, r_even=1, r_even2=2, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000000000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000000210</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000212</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100012212</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110012212</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>012221201</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200010211</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201012211</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201212211</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000110222</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3764 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            00     01   02     10   11   12     20   21   22\n",
       "000000000  1.0    1.0  1.0    1.0  1.0  1.0    1.0  1.0  1.0\n",
       "000000210  0.0    0.0  0.0    0.0  0.0  0.0    0.0  0.0  0.0\n",
       "100000212  0.0    0.0  0.0    0.0  0.0  0.0    0.0  0.0  0.0\n",
       "100012212  0.0    0.0  0.0    0.0  0.0  0.0    0.0  0.0  0.0\n",
       "110012212  0.0  100.0  0.0    0.0  0.0  0.0    0.0  0.0  0.0\n",
       "...        ...    ...  ...    ...  ...  ...    ...  ...  ...\n",
       "012221201  0.0    0.0  0.0    0.0  0.0  0.0    0.0  0.0  0.0\n",
       "200010211  0.0    0.0  0.0    0.0  0.0  0.0    0.0  0.0  0.0\n",
       "201012211  0.0    0.0  0.0    0.0  0.0  0.0    0.0  0.0  0.0\n",
       "201212211  0.0    0.0  0.0  100.0  0.0  0.0    0.0  0.0  0.0\n",
       "000110222  0.0    0.0  0.0    0.0  0.0  0.0  100.0  0.0  0.0\n",
       "\n",
       "[3764 rows x 9 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EiVQ3zajRlOC",
    "outputId": "d288c90e-e776-4b2d-ca76-d31a0a421ffa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................tot vict : 279  tot lose : 14647  tot even : 15074\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(size=3)\n",
    "game = Game('Vic', agent, n_dim=2, size=3)\n",
    "sarsa(game, agent, deterministic_policy, alpha=1.0, alpha_factor=0.999, gamma=0.95, epsilon=1.0, epsilon_factor=0.9998, \\\n",
    "      r_win=100, r_lose=0, r_even=1, r_even2=2, num_episodes=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................tot vict : 182  tot lose : 9753  tot even : 10065\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(size=3)\n",
    "game = Game('Vic', agent, n_dim=2, size=3)\n",
    "sarsa(game, agent, deterministic_policy, alpha=1.0, alpha_factor=0.999, gamma=0.95, epsilon=1.0, epsilon_factor=0.9998, \\\n",
    "      r_win=100, r_lose=0, r_even=1, r_even2=2, num_episodes=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..........tot vict : 99  tot lose : 4841  tot even : 5060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "agent = Agent(size=3)\n",
    "game = Game('Vic', agent, n_dim=2, size=3)\n",
    "q_learning(game, agent, random_policy, alpha=1.0, alpha_factor=0.99999, gamma=0.8, epsilon=1.0, epsilon_factor=0.99997, \\\n",
    "           r_win=10, r_lose=-5, r_even=1, r_even2=2, num_episodes=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(agent.q_array.sum(axis=1) != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . \n",
      ". . . \n",
      ". . . \n",
      "\n",
      "Agent plays : (2, 2) \n",
      "\n",
      ". . . \n",
      ". . . \n",
      ". . X \n",
      "\n",
      "Agent plays : (0, 0) \n",
      "\n",
      "O . . \n",
      ". . . \n",
      ". . X \n",
      "\n",
      "Agent plays : (2, 1) \n",
      "\n",
      "O . . \n",
      ". . . \n",
      ". X X \n",
      "\n",
      "Agent plays : (0, 1) \n",
      "\n",
      "O O . \n",
      ". . . \n",
      ". X X \n",
      "\n",
      "Agent plays : (2, 0) \n",
      "\n",
      "O O . \n",
      ". . . \n",
      "X X X \n",
      "\n",
      "Game over. Score : (1, 0)\n",
      "<__main__.Agent object at 0x0000010F194A1850> wins !\n"
     ]
    }
   ],
   "source": [
    "agent2 = Agent(size=3, policy=random_policy)\n",
    "game = Game(agent, agent2, n_dim=2, size=3)\n",
    "game.play_a_game()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RL_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
